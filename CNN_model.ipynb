{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comet_ml at the top of your file\n",
    "# from comet_ml import Experiment\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# from matplotlib import style\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import swifter\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras as k\n",
    "import tensorflow as tf\n",
    "# import torch\n",
    "from scipy import stats\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "# Create an experiment with your api key\n",
    "# experiment = Experiment(\n",
    "#     api_key=\"BKac2uRt0FMAlheXf6HClaZhD\",\n",
    "#     project_name=\"general\",\n",
    "#     workspace=\"sarthak7034\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/s/sarthak_7034/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('gpu')))\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to plot the confusion matrix (code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "    \n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, cm[i, j],\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "data = pd.read_csv(\"./all_data.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    #Tokenize\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # remove stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]     # Experiment with removing it!!!\n",
    "    #Remove non alphanumerica characters\n",
    "    words = [word for word in tokens if word.isalpha()]    \n",
    "    return words\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154a0822d4e84c739424971df5461553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/20015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61877b2736f14dada16bc3843c0275c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/20015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying lamdba function to clean text\n",
    "data['text'] = data.swifter.apply(lambda row: clean_text(row['text']), axis=1)\n",
    "#Clean title\n",
    "data['title'] = data.swifter.apply(lambda row: clean_text(row['title']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing row with only text\n",
    "data = data[data['title'].str.len() >= 1]\n",
    "data = data[data['text'].str.len() >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].map(len) >= 1]\n",
    "#Reset index\n",
    "data = data.reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.loc[:,['anger','anticipation','disgust','fear','joy','sadness','surprise','trust','negative','positive']]\n",
    "\n",
    "# # Zero Score Dataframe Normalization\n",
    "# MaxMin_df = stats.zscore(df1)\n",
    "# # MaxMin_df = scaler.fit_transform(df1)\n",
    "# MaxMin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the new dataset which we will be working with\n",
    "df2 = data.loc[:,['title','text','type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[At, Donald, Trump, Properties, Showcase, Bran...</td>\n",
       "      <td>[They, stood, line, Trump, Tower, sometimes, h...</td>\n",
       "      <td>real</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Trump, Foundation, Tells, New, York, It, Has,...</td>\n",
       "      <td>[Donald, J, Trump, foundation, informed, Attor...</td>\n",
       "      <td>real</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Donald, Trump, Prepares, White, House, Move, ...</td>\n",
       "      <td>[President, Donald, J, Trump, White, House, ou...</td>\n",
       "      <td>real</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Luring, Chinese, Investors, With, Trump, Name...</td>\n",
       "      <td>[An, investment, pitch, new, Texas, hotel, try...</td>\n",
       "      <td>real</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Melania, Barron, Trump, Won, Immediately, Mov...</td>\n",
       "      <td>[President, Donald, J, Trump, wife, Melania, s...</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  [At, Donald, Trump, Properties, Showcase, Bran...   \n",
       "1  [Trump, Foundation, Tells, New, York, It, Has,...   \n",
       "2  [Donald, Trump, Prepares, White, House, Move, ...   \n",
       "3  [Luring, Chinese, Investors, With, Trump, Name...   \n",
       "4  [Melania, Barron, Trump, Won, Immediately, Mov...   \n",
       "\n",
       "                                                text  type  anger  \\\n",
       "0  [They, stood, line, Trump, Tower, sometimes, h...  real      9   \n",
       "1  [Donald, J, Trump, foundation, informed, Attor...  real      6   \n",
       "2  [President, Donald, J, Trump, White, House, ou...  real      5   \n",
       "3  [An, investment, pitch, new, Texas, hotel, try...  real     11   \n",
       "4  [President, Donald, J, Trump, wife, Melania, s...  real      1   \n",
       "\n",
       "   anticipation  disgust  fear  joy  sadness  surprise  trust  negative  \\\n",
       "0            21        5     6   20        5        14     30        14   \n",
       "1             7        2     4    4        4         5      9         8   \n",
       "2            19        6     8   15        8         6     26        15   \n",
       "3            14        6    10   10       10         6     32        24   \n",
       "4            12        0     3    6        2         4     14         4   \n",
       "\n",
       "   positive  \n",
       "0        52  \n",
       "1        15  \n",
       "2        34  \n",
       "3        43  \n",
       "4        25  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge = pd.concat([df2,df1],axis=1)\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether any values are missing\n",
    "# df2.isnull().sum()\n",
    "# df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "fake    11731\n",
      "real     8074\n",
      "Name: title, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How many fake and real articles?\n",
    "print(df_merge.groupby(['type'])['title'].count())\n",
    "df_merge.groupby(['type'])['title'].count().plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed = KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(model_embed.key_to_index.keys())\n",
    "print(vocab_length)                                         #Input dimension size(3000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9efe93569c14bd8ba30bb8757c00249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd68141b5aba43fda9c45aa73061b16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying it to the columns \n",
    "\n",
    "df_merge['title'] = df_merge.swifter.apply(lambda r: [model_embed.key_to_index[x] for x in r['title'] if x in model_embed.key_to_index], axis=1)\n",
    "df_merge['text'] = df_merge.swifter.apply(lambda r: [model_embed.key_to_index[x] for x in r['text'] if x in model_embed.key_to_index], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-hot encoding\n",
    "# encoder = LabelBinarizer().fit(list(df2['type']))\n",
    "# df2['type'] = df2.swifter.apply(lambda r: encoder.transform([r['type']])[0], axis=1)\n",
    "\n",
    "variable_name = {'fake' : 0 , 'real' : 1 }\n",
    "df_merge['type'] = df_merge['type'].map(variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19672, 13)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge = df_merge[df_merge['text'].map(len) >= 1]\n",
    "#Reset index\n",
    "df_merge = df_merge.reset_index().drop(\"index\", axis=1)\n",
    "df_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting all to a new dataframe\n",
    "df3 = df_merge.loc[:,['title','text','type','anger','anticipation','disgust','fear','joy','sadness','surprise','trust','negative','positive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the highest length of text and title:\n",
    "max_tokens_text = 13000\n",
    "max_tokens_title = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['text'] = list(pad_sequences((df_merge['text']), maxlen=max_tokens_text,padding='post'))\n",
    "df3['title'] = list(pad_sequences(df_merge['title'], maxlen=max_tokens_title,padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19672, 13)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python object serialized\n",
    "df3.to_pickle('./model_data.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[332, 6117, 13034, 11062, 20257, 9193, 446, 0,...</td>\n",
       "      <td>[128, 2379, 318, 13034, 9413, 1631, 242, 957, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[13034, 2124, 44625, 442, 5353, 51, 7754, 8808...</td>\n",
       "      <td>[6117, 4883, 13034, 3113, 2826, 4605, 1927, 33...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6117, 13034, 49031, 1790, 659, 11977, 553, 94...</td>\n",
       "      <td>[446, 6117, 4883, 13034, 1790, 659, 16780, 177...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[176192, 1035, 3789, 316, 13034, 12441, 3337, ...</td>\n",
       "      <td>[741, 615, 1924, 65, 675, 1639, 469, 8793, 112...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[282726, 14896, 13034, 26994, 17785, 11977, 17...</td>\n",
       "      <td>[446, 6117, 4883, 13034, 783, 282726, 831, 148...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  [332, 6117, 13034, 11062, 20257, 9193, 446, 0,...   \n",
       "1  [13034, 2124, 44625, 442, 5353, 51, 7754, 8808...   \n",
       "2  [6117, 13034, 49031, 1790, 659, 11977, 553, 94...   \n",
       "3  [176192, 1035, 3789, 316, 13034, 12441, 3337, ...   \n",
       "4  [282726, 14896, 13034, 26994, 17785, 11977, 17...   \n",
       "\n",
       "                                                text  type  anger  \\\n",
       "0  [128, 2379, 318, 13034, 9413, 1631, 242, 957, ...     1      9   \n",
       "1  [6117, 4883, 13034, 3113, 2826, 4605, 1927, 33...     1      6   \n",
       "2  [446, 6117, 4883, 13034, 1790, 659, 16780, 177...     1      5   \n",
       "3  [741, 615, 1924, 65, 675, 1639, 469, 8793, 112...     1     11   \n",
       "4  [446, 6117, 4883, 13034, 783, 282726, 831, 148...     1      1   \n",
       "\n",
       "   anticipation  disgust  fear  joy  sadness  surprise  trust  negative  \\\n",
       "0            21        5     6   20        5        14     30        14   \n",
       "1             7        2     4    4        4         5      9         8   \n",
       "2            19        6     8   15        8         6     26        15   \n",
       "3            14        6    10   10       10         6     32        24   \n",
       "4            12        0     3    6        2         4     14         4   \n",
       "\n",
       "   positive  \n",
       "0        52  \n",
       "1        15  \n",
       "2        34  \n",
       "3        43  \n",
       "4        25  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pickle = pd.read_pickle('./model_data.pickle')\n",
    "data_pickle = data_pickle.dropna()\n",
    "data_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train - test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_pickle[['text', 'title','anger','anticipation','disgust','fear','joy','sadness','surprise','trust','negative','positive']], data_pickle['type'], test_size=0.2, random_state=42)\n",
    "\n",
    "#Train - valid\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = X_train.iloc[:,2:]\n",
    "# Zero Score Dataframe Normalization\n",
    "# MaxMin_df = stats.zscore(df1)\n",
    "# MaxMin_df.head()\n",
    "MaxMin_df_train = scaler.fit_transform(dftrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = X_test.iloc[:,2:]\n",
    "MaxMin_df_test = scaler.fit_transform(dftest)\n",
    "\n",
    "dfvalid = X_valid.iloc[:,2:]\n",
    "MaxMin_df_valid = scaler.fit_transform(dfvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the highest length of text and title:\n",
    "max_tokens_text = 13000\n",
    "max_tokens_title = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fit = [np.asarray(X_train['title'].tolist()), \n",
    "             np.asarray(X_train['text'].tolist()),\n",
    "            MaxMin_df_train\n",
    "            ]\n",
    "\n",
    "test_fit = [np.asarray(X_test['title'].tolist()), \n",
    "             np.asarray(X_test['text'].tolist()),\n",
    "            MaxMin_df_test\n",
    "            ]    \n",
    "\n",
    "valid_fit = [np.asarray(X_valid['title'].tolist()), \n",
    "             np.asarray(X_valid['text'].tolist()),\n",
    "            MaxMin_df_valid\n",
    "            ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "seed(50)\n",
    "tf.random.set_seed(50)\n",
    "k.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 48)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 13000)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 48, 300)      900000000   ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 13000, 300)   900000000   ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 23, 32)       38432       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 6499, 64)     76864       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 11, 32)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 1624, 64)    0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 352)          0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 103936)       0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 10)           0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           17650       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 100)          10393700    ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10)           110         ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 160)          0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]',                \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 160)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 50)           8050        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            51          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,810,534,857\n",
      "Trainable params: 10,534,857\n",
      "Non-trainable params: 1,800,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_title\n",
    "title_input = k.layers.Input(shape=(max_tokens_title,))\n",
    "inp1 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(title_input)\n",
    "# x1 = k.layers.Dropout(0.2)(inp1)\n",
    "\n",
    "x1 = k.layers.Conv1D(filters = 32, kernel_size=4, strides=2, activation='relu')(inp1)\n",
    "x1 = k.layers.MaxPool1D(pool_size = 2)(x1)\n",
    "x1 = k.layers.Flatten()(x1)\n",
    "x1 = k.layers.Dense(50, activation='relu', kernel_regularizer='l2')(x1)\n",
    "\n",
    "\n",
    "#input_Text\n",
    "text_input = k.layers.Input(shape=(max_tokens_text,))\n",
    "inp2 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(text_input)\n",
    "# x2 = k.layers.Dropout(0.2)(inp2)\n",
    "\n",
    "x2 = k.layers.Conv1D(filters = 64, kernel_size = 4, strides = 2, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4)(x2)\n",
    "\n",
    "\n",
    "# x2 = k.layers.Conv1D(filters = 80, kernel_size = 16, strides = 4, activation='relu')(inp2)\n",
    "# x2 = k.layers.MaxPool1D(pool_size = 4)(x2)\n",
    "x2 = k.layers.Flatten()(x2)\n",
    "x2 = k.layers.Dense(100, activation='relu', kernel_regularizer='l2')(x2)\n",
    "\n",
    "\n",
    "#Sentiments-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "sentiment_input = k.layers.Input(shape=(10,))\n",
    "x3 = k.layers.Flatten()(sentiment_input)\n",
    "x3= k.layers.Dense(10, activation='relu', kernel_regularizer='l2')(x3)\n",
    "\n",
    "#Merge-------------------------------------------------------------------------------------------------------------\n",
    "# x = k.layers.concatenate([x1,x2])\n",
    "x = k.layers.concatenate([x1,x2,x3])\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "\n",
    "#Common part-------------------------------------------------------------------------------------------------------\n",
    "# x = k.layers.Dense(50, activation='relu')(x)\n",
    "# x = k.layers.Dropout(0.2)(x)\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "out = k.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "#Build model\n",
    "# model = k.models.Model(inputs=[title_input, text_input], outputs=[out])\n",
    "model = k.models.Model(inputs=[title_input, text_input,sentiment_input], outputs=[out])\n",
    "model.compile(k.optimizers.RMSprop(), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "99/99 [==============================] - 214s 2s/step - loss: 1.0387 - acc: 0.6969 - val_loss: 0.5933 - val_acc: 0.8078\n",
      "Epoch 2/30\n",
      "99/99 [==============================] - 210s 2s/step - loss: 0.5514 - acc: 0.8187 - val_loss: 0.4820 - val_acc: 0.8536\n",
      "Epoch 3/30\n",
      "99/99 [==============================] - 213s 2s/step - loss: 0.4414 - acc: 0.8646 - val_loss: 0.4726 - val_acc: 0.8393\n",
      "Epoch 4/30\n",
      "99/99 [==============================] - 212s 2s/step - loss: 0.3635 - acc: 0.9017 - val_loss: 0.3386 - val_acc: 0.9133\n",
      "Epoch 5/30\n",
      "99/99 [==============================] - 209s 2s/step - loss: 0.3079 - acc: 0.9224 - val_loss: 0.3609 - val_acc: 0.8914\n",
      "Epoch 6/30\n",
      "99/99 [==============================] - 210s 2s/step - loss: 0.2654 - acc: 0.9388 - val_loss: 0.3028 - val_acc: 0.9215\n",
      "Epoch 7/30\n",
      "99/99 [==============================] - 211s 2s/step - loss: 0.2220 - acc: 0.9542 - val_loss: 0.2646 - val_acc: 0.9323\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=128, epochs=30,\n",
    "        callbacks = [k.callbacks.EarlyStopping(monitor='val_loss', patience=3,min_delta = 0.1)], \n",
    "        validation_data=(valid_fit, np.array(y_valid.tolist()))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      2354\n",
      "           1       0.96      0.88      0.92      1581\n",
      "\n",
      "    accuracy                           0.94      3935\n",
      "   macro avg       0.94      0.93      0.93      3935\n",
      "weighted avg       0.94      0.94      0.94      3935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(np.array(y_test.tolist()), test_pred.round())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for testing data is 0.9341804320203304\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy for testing data is\", np.mean(test_pred.round() == np.array(y_test.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "seed(50)\n",
    "tf.random.set_seed(50)\n",
    "k.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 48)]         0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 13000)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)        (None, 48, 300)      900000000   ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, 13000, 300)   900000000   ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 23, 32)       38432       ['embedding_8[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 6499, 64)     76864       ['embedding_9[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPooling1D)  (None, 11, 32)      0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPooling1D)  (None, 1624, 64)    0           ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " flatten_12 (Flatten)           (None, 352)          0           ['max_pooling1d_8[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_13 (Flatten)           (None, 103936)       0           ['max_pooling1d_9[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)           (None, 10)           0           ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 50)           17650       ['flatten_12[0][0]']             \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 100)          10393700    ['flatten_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 10)           110         ['flatten_14[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 160)          0           ['dense_20[0][0]',               \n",
      "                                                                  'dense_21[0][0]',               \n",
      "                                                                  'dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 160)          0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 50)           8050        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 50)           0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 5)            255         ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,810,535,061\n",
      "Trainable params: 10,535,061\n",
      "Non-trainable params: 1,800,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_title\n",
    "title_input = k.layers.Input(shape=(max_tokens_title,))\n",
    "inp1 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(title_input)\n",
    "\n",
    "x1 = k.layers.Conv1D(filters = 32, kernel_size=4, strides=2, activation='relu')(inp1)\n",
    "x1 = k.layers.MaxPool1D(pool_size = 2)(x1)\n",
    "x1 = k.layers.Flatten()(x1)\n",
    "x1 = k.layers.Dense(50, activation='relu',  kernel_regularizer='l2')(x1)\n",
    "\n",
    "#input_content\n",
    "text_input = k.layers.Input(shape=(max_tokens_text,))\n",
    "inp2 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(text_input)\n",
    "\n",
    "x2 = k.layers.Conv1D(filters = 64, kernel_size = 4, strides = 2, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4 )(x2)\n",
    "\n",
    "#Added\n",
    "# x2 = k.layers.Conv1D(filters = 80, kernel_size = 16, strides = 4, activation='relu')(inp2)\n",
    "# x2 = k.layers.MaxPool1D(pool_size = 4, )(x2)\n",
    "x2 = k.layers.Flatten()(x2)\n",
    "x2 = k.layers.Dense(100, activation='relu', kernel_regularizer='l2')(x2)\n",
    "\n",
    "sentiment_input = k.layers.Input(shape=(10,))\n",
    "x3 = k.layers.Flatten()(sentiment_input)\n",
    "x3= k.layers.Dense(10, activation='relu', kernel_regularizer='l2')(x3)\n",
    "\n",
    "\n",
    "#Merge\n",
    "# x = k.layers.concatenate([x1, x2])\n",
    "x = k.layers.concatenate([x1,x2,x3])\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "\n",
    "#Common part\n",
    "# x = k.layers.Dense(50, activation='relu')(x)\n",
    "# x = k.layers.Dropout(0.2)(x)\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "out = k.layers.Dense(5, activation='softmax')(x)\n",
    "\n",
    "\n",
    "#Build model\n",
    "# model = k.models.Model(inputs=[title_input, text_input], outputs=[out])\n",
    "model = k.models.Model(inputs=[title_input, text_input,sentiment_input], outputs=[out])\n",
    "model.compile(k.optimizers.RMSprop(), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=128, epochs=30,\n",
    "        callbacks = [k.callbacks.EarlyStopping(monitor='val_loss', patience=3,min_delta = 0.1)], \n",
    "        validation_data=(valid_fit, np.array(y_valid.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted indices (ValueError: Classification metrics can't handle a mix of binary and continuous-multioutput targets)\n",
    "test_pred = np.argmax(test_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95      2354\n",
      "           1       0.97      0.86      0.91      1581\n",
      "\n",
      "    accuracy                           0.93      3935\n",
      "   macro avg       0.94      0.92      0.93      3935\n",
      "weighted avg       0.94      0.93      0.93      3935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(np.array(y_test.tolist()), test_pred.round())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for testing data is 0.9428208386277002\n",
      "The accuracy for training data is 0.9998411311462387\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy for testing data is\", np.mean(test_pred.round() == np.array(y_test.tolist())) )\n",
    "print(\"The accuracy for training data is\", np.mean(np.argmax(model.predict(train_fit),1) == np.array(y_train.tolist())) )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c79634922abe25a0b9d55bc2eb6abac259bd6c8bbb4022582053c5e5bdfcda26"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
