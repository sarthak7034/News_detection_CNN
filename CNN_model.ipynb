{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5076"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comet_ml at the top of your file\n",
    "# from comet_ml import Experiment\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# from matplotlib import style\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import swifter\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras as k\n",
    "import tensorflow as tf\n",
    "# import torch\n",
    "from scipy import stats\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "# Create an experiment with your api key\n",
    "# experiment = Experiment(\n",
    "#     api_key=\"BKac2uRt0FMAlheXf6HClaZhD\",\n",
    "#     project_name=\"general\",\n",
    "#     workspace=\"sarthak7034\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/s/sarthak_7034/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('gpu')))\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the confusion matrix (code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "data = pd.read_csv(\"./all_data.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = stop_words.remove('not')\n",
    "def clean_text(text):\n",
    "    #Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    #Tokenize\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # remove stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]     # Experiment with removing it!!!\n",
    "    #Remove non alphanumerica characters\n",
    "    words = [word for word in tokens if word.isalpha()]    \n",
    "    return words\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c52217295364ef1a3aa8c82364fde0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/20015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5a5961a0634d2a945ce54307f38c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/20015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying lamdba function to clean text\n",
    "data['text'] = data.swifter.apply(lambda row: clean_text(row['text']), axis=1)\n",
    "#Clean title\n",
    "data['title'] = data.swifter.apply(lambda row: clean_text(row['title']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing row with only text\n",
    "data = data[data['title'].str.len() >= 1]\n",
    "data = data[data['text'].str.len() >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = data.loc[:,['anger','anticipation','disgust','fear','joy','sadness','surprise','trust','negative','positive']]\n",
    "\n",
    "# # Zero Score Dataframe Normalization\n",
    "# MaxMin_df = stats.zscore(df1)\n",
    "# MaxMin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the new dataset which we will be working with\n",
    "df2 = data.loc[:,['title','text','type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether any values are missing\n",
    "# df2.isnull().sum()\n",
    "# df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[At, Donald, Trump, Properties, Showcase, Bran...</td>\n",
       "      <td>[They, stood, line, Trump, Tower, sometimes, h...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Trump, Foundation, Tells, New, York, It, Has,...</td>\n",
       "      <td>[Donald, J, Trump, foundation, informed, Attor...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Donald, Trump, Prepares, White, House, Move, ...</td>\n",
       "      <td>[President, Donald, J, Trump, White, House, ou...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Luring, Chinese, Investors, With, Trump, Name...</td>\n",
       "      <td>[An, investment, pitch, new, Texas, hotel, try...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Melania, Barron, Trump, Won, Immediately, Mov...</td>\n",
       "      <td>[President, Donald, J, Trump, wife, Melania, s...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19800</th>\n",
       "      <td>[Alabama, Lawmaker, Same, Couples, Don, Deserv...</td>\n",
       "      <td>[Most, conservatives, oppose, marriage, equali...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19801</th>\n",
       "      <td>[GOP, Senator, David, Perdue, Jokes, About, Pr...</td>\n",
       "      <td>[The, freshman, senator, Georgia, quoted, scri...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19802</th>\n",
       "      <td>[State, Department, says, find, emails, Clinto...</td>\n",
       "      <td>[The, State, Department, told, Republican, Nat...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19803</th>\n",
       "      <td>[In, Ethiopia, Obama, seeks, progress, peace, ...</td>\n",
       "      <td>[ADDIS, ABABA, Ethiopia, Obama, convened, meet...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19804</th>\n",
       "      <td>[Jeb, Bush, Is, Suddenly, Attacking, Trump, He...</td>\n",
       "      <td>[Jeb, Bush, Is, Suddenly, Attacking, Trump, He...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19805 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      [At, Donald, Trump, Properties, Showcase, Bran...   \n",
       "1      [Trump, Foundation, Tells, New, York, It, Has,...   \n",
       "2      [Donald, Trump, Prepares, White, House, Move, ...   \n",
       "3      [Luring, Chinese, Investors, With, Trump, Name...   \n",
       "4      [Melania, Barron, Trump, Won, Immediately, Mov...   \n",
       "...                                                  ...   \n",
       "19800  [Alabama, Lawmaker, Same, Couples, Don, Deserv...   \n",
       "19801  [GOP, Senator, David, Perdue, Jokes, About, Pr...   \n",
       "19802  [State, Department, says, find, emails, Clinto...   \n",
       "19803  [In, Ethiopia, Obama, seeks, progress, peace, ...   \n",
       "19804  [Jeb, Bush, Is, Suddenly, Attacking, Trump, He...   \n",
       "\n",
       "                                                    text  type  \n",
       "0      [They, stood, line, Trump, Tower, sometimes, h...  real  \n",
       "1      [Donald, J, Trump, foundation, informed, Attor...  real  \n",
       "2      [President, Donald, J, Trump, White, House, ou...  real  \n",
       "3      [An, investment, pitch, new, Texas, hotel, try...  real  \n",
       "4      [President, Donald, J, Trump, wife, Melania, s...  real  \n",
       "...                                                  ...   ...  \n",
       "19800  [Most, conservatives, oppose, marriage, equali...  real  \n",
       "19801  [The, freshman, senator, Georgia, quoted, scri...  real  \n",
       "19802  [The, State, Department, told, Republican, Nat...  real  \n",
       "19803  [ADDIS, ABABA, Ethiopia, Obama, convened, meet...  real  \n",
       "19804  [Jeb, Bush, Is, Suddenly, Attacking, Trump, He...  real  \n",
       "\n",
       "[19805 rows x 3 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2[df2['text'].map(len) >= 1]\n",
    "#Reset index\n",
    "df2 = df2.reset_index().drop(\"index\", axis=1)\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "fake    11731\n",
      "real     8074\n",
      "Name: title, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How many fake and real articles?\n",
    "print(df2.groupby(['type'])['title'].count())\n",
    "df2.groupby(['type'])['title'].count().plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed = KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(model_embed.key_to_index.keys())\n",
    "print(vocab_length)                                         #Input dimension size(3000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e406de290c4a4c4e89892d39fbf131af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083c39e51f1e4795a191e2f5335e2f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying it to the columns \n",
    "\n",
    "df2['title'] = df2.swifter.apply(lambda r: [model_embed.key_to_index[x] for x in r['title'] if x in model_embed.key_to_index], axis=1)\n",
    "df2['text'] = df2.swifter.apply(lambda r: [model_embed.key_to_index[x] for x in r['text'] if x in model_embed.key_to_index], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ddd0a6b78b4000b630fc2dcc6ce331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "encoder = LabelBinarizer().fit(list(df2['type']))\n",
    "df2['type'] = df2.swifter.apply(lambda r: encoder.transform([r['type']])[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting all to a new dataframe\n",
    "df3 = df2.loc[:,['title','text','type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def pad_array(array, token_len):\n",
    "#     diff_token = token_len - len(array)\n",
    "#     if(diff_token < 0):\n",
    "#         array = array[:token_len] \n",
    "#     else:\n",
    "#         array += [0]*diff_token \n",
    "        \n",
    "#     return array \n",
    "\n",
    "# tokens_numbers_text = df2.swifter.apply(lambda row: len(row['text']), axis = 1)\n",
    "# max_tokens_text = int(np.mean(tokens_numbers_text) + 2 * np.std(tokens_numbers_text))\n",
    "\n",
    "# tokens_numbers_title = df2.swifter.apply(lambda row: len(row['title']), axis = 1)\n",
    "# max_tokens_title = int(np.mean(tokens_numbers_title) + 2 * np.std(tokens_numbers_title))\n",
    "# print(max_tokens_text,max_tokens_title)    \n",
    "\n",
    "# df3['text'] = df2.swifter.apply(lambda r: pad_array(r['text'], max_tokens_text) , axis=1)\n",
    "# df3['title'] = df2.swifter.apply(lambda r: pad_array(r['title'], max_tokens_title) , axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the highest length of text and title:\n",
    "max_tokens_text = 13000\n",
    "max_tokens_title = 48\n",
    "\n",
    "df3['text'] = list(pad_sequences((df2['text']), maxlen=max_tokens_text,padding='post'))\n",
    "df3['title'] = list(pad_sequences(df2['title'], maxlen=max_tokens_title,padding='post'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving it as Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python object serialized\n",
    "df3.to_pickle('./model_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pickle = pd.read_pickle('./model_data.pickle')\n",
    "data_pickle = data_pickle.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train - test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_pickle[['text', 'title']], data_pickle['type'], test_size=0.2, random_state=42)\n",
    "\n",
    "#Train - valid\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42)\n",
    " \n",
    "\n",
    "train_fit = [np.asarray(X_train['title'].tolist()), \n",
    "             np.asarray(X_train['text'].tolist())\n",
    "             ]\n",
    "\n",
    "valid_fit = [np.asarray(X_valid['title'].tolist()), \n",
    "             np.asarray(X_valid['text'].tolist())\n",
    "             ]\n",
    "\n",
    "test_fit = [np.asarray(X_test['title'].tolist()), \n",
    "             np.asarray(X_test['text'].tolist())\n",
    "             ]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "seed(50)\n",
    "tf.random.set_seed(50)\n",
    "k.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 48)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 13000)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 48, 300)      900000000   ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 13000, 300)   900000000   ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 23, 5)        6005        ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 3247, 80)     384080      ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " Pool1Title (MaxPooling1D)      (None, 11, 5)        0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 811, 80)     0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 55)           0           ['Pool1Title[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 64880)        0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " Dense1Title (Dense)            (None, 50)           2800        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " Dense1Text (Dense)             (None, 100)          6488100     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 150)          0           ['Dense1Title[0][0]',            \n",
      "                                                                  'Dense1Text[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           7550        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 50)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 50)           2550        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            51          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,806,891,136\n",
      "Trainable params: 6,891,136\n",
      "Non-trainable params: 1,800,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_title\n",
    "title_input = k.layers.Input(shape=(max_tokens_title,))\n",
    "# inp1 = k.layers.Embedding(input_dim=3000000,output_dim=300)(title_input)\n",
    "inp1 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(title_input)\n",
    "\n",
    "#Added\n",
    "x1 = k.layers.Conv1D(filters = 5, kernel_size=4, strides=2, activation='relu')(inp1)\n",
    "x1 = k.layers.MaxPool1D(pool_size = 2, name='Pool1Title')(x1)\n",
    "x1 = k.layers.Flatten()(x1)\n",
    "x1 = k.layers.Dense(50, activation='relu', name='Dense1Title', kernel_regularizer='l2')(x1)\n",
    "\n",
    "#input_Text\n",
    "text_input = k.layers.Input(shape=(max_tokens_text,))\n",
    "# inp2 = k.layers.Embedding(input_dim=3000000,output_dim=300)(text_input)\n",
    "inp2 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(text_input)\n",
    "\n",
    "x2 = k.layers.Conv1D(filters = 40, kernel_size = 16, strides = 2, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4)(x2)\n",
    "\n",
    "#Added\n",
    "x2 = k.layers.Conv1D(filters = 80, kernel_size = 16, strides = 4, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4)(x2)\n",
    "x2 = k.layers.Flatten()(x2)\n",
    "x2 = k.layers.Dense(100, activation='relu', kernel_regularizer='l2', name='Dense1Text')(x2)\n",
    "\n",
    "\n",
    "#Merge\n",
    "x = k.layers.concatenate([x1, x2])\n",
    "\n",
    "#Common part\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "out = k.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "#Build model\n",
    "model = k.models.Model(inputs=[title_input, text_input], outputs=[out])\n",
    "\n",
    "model.compile(k.optimizers.Nadam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #input_title\n",
    "# title_input = k.layers.Input(shape=(max_tokens_title,))\n",
    "# inp1 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(title_input)\n",
    "\n",
    "# x = k.layers.Conv1D(filters = 5, kernel_size=4, strides=2, activation='relu')(inp1)\n",
    "# x = k.layers.MaxPool1D(pool_size = 2)(x)\n",
    "# x = k.layers.Flatten()(x)\n",
    "# x = k.layers.Dense(50, activation='relu',  kernel_regularizer='l2')(x)\n",
    "\n",
    "# #input_content\n",
    "# text_input = k.layers.Input(shape=(max_tokens_text,))\n",
    "# inp2 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(text_input)\n",
    "\n",
    "# x2 = k.layers.Conv1D(filters = 40, kernel_size = 16, strides = 2, activation='relu')(inp2)\n",
    "# x2 = k.layers.MaxPool1D(pool_size = 4 )(x2)\n",
    "\n",
    "# #Added\n",
    "# x2 = k.layers.Conv1D(filters = 80, kernel_size = 16, strides = 4, activation='relu')(inp2)\n",
    "# x2 = k.layers.MaxPool1D(pool_size = 4, )(x2)\n",
    "# x2 = k.layers.Flatten()(x2)\n",
    "# x2 = k.layers.Dense(100, activation='relu', kernel_regularizer='l2')(x2)\n",
    "\n",
    "\n",
    "# #Merge\n",
    "# x = k.layers.concatenate([x, x2])\n",
    "\n",
    "# #Common part\n",
    "# x = k.layers.Dense(50, activation='relu')(x)\n",
    "# x = k.layers.Dropout(0.2)(x)\n",
    "# x = k.layers.Dense(50, activation='relu')(x)\n",
    "# x = k.layers.Dropout(0.2)(x)\n",
    "# out = k.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "\n",
    "# #Build model\n",
    "# model = k.models.Model(inputs=[title_input, text_input], outputs=[out])\n",
    "\n",
    "# model.compile(k.optimizers.RMSprop(), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 302s 3s/step - loss: 1.0038 - acc: 0.7038 - val_loss: 0.6280 - val_acc: 0.8299\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 300s 3s/step - loss: 0.5092 - acc: 0.8518 - val_loss: 0.4730 - val_acc: 0.8447\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 300s 3s/step - loss: 0.4959 - acc: 0.8811 - val_loss: 1.4863 - val_acc: 0.6589\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 300s 3s/step - loss: 0.5696 - acc: 0.9034 - val_loss: 0.5262 - val_acc: 0.8741\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 301s 3s/step - loss: 0.3958 - acc: 0.9398 - val_loss: 0.4401 - val_acc: 0.8959\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 302s 3s/step - loss: 0.2354 - acc: 0.9718 - val_loss: 0.4278 - val_acc: 0.8700\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 303s 3s/step - loss: 0.1451 - acc: 0.9886 - val_loss: 0.3780 - val_acc: 0.9006\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 299s 3s/step - loss: 0.1237 - acc: 0.9878 - val_loss: 0.5131 - val_acc: 0.8826\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 298s 3s/step - loss: 0.1111 - acc: 0.9864 - val_loss: 0.4063 - val_acc: 0.8892\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 299s 3s/step - loss: 0.0738 - acc: 0.9976 - val_loss: 0.3807 - val_acc: 0.9116\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 303s 3s/step - loss: 0.1419 - acc: 0.9871 - val_loss: 0.3837 - val_acc: 0.9094\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=128, epochs=20,\n",
    "        callbacks = [k.callbacks.EarlyStopping(monitor='val_loss', patience=4)], \n",
    "        validation_data=(valid_fit, np.array(y_valid.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9136581671295128"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array(y_test.tolist()), test_pred.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      2374\n",
      "           1       0.90      0.88      0.89      1587\n",
      "\n",
      "    accuracy                           0.91      3961\n",
      "   macro avg       0.91      0.91      0.91      3961\n",
      "weighted avg       0.91      0.91      0.91      3961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(np.array(y_test.tolist()), test_pred.round())\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c79634922abe25a0b9d55bc2eb6abac259bd6c8bbb4022582053c5e5bdfcda26"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
