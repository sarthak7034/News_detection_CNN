{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comet_ml at the top of your file\n",
    "# from comet_ml import Experiment\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# from matplotlib import style\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import swifter\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras as k\n",
    "import tensorflow as tf\n",
    "# import torch\n",
    "from scipy import stats\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Create an experiment with your api key\n",
    "# experiment = Experiment(\n",
    "#     api_key=\"BKac2uRt0FMAlheXf6HClaZhD\",\n",
    "#     project_name=\"general\",\n",
    "#     workspace=\"sarthak7034\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/s/sarthak_7034/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the confusion matrix (code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "data = pd.read_csv(\"./all_data.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    #Tokenize\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # remove stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words] \n",
    "    #Remove non alphanumerica characters\n",
    "    words = [word for word in tokens if word.isalpha()]    \n",
    "    return words\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4204753e4e6e4b54b86b08f78609be6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5456005c834d9f84ffff53f5d01ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying lamdba function to clean text\n",
    "data['text'] = data.swifter.apply(lambda row: clean_text(row['text']), axis=1)\n",
    "#Clean title\n",
    "data['title'] = data.swifter.apply(lambda row: clean_text(row['title']), axis=1)\n",
    "\n",
    "#Replacing columns with cleaned ones\n",
    "data = data[data['title'].str.len() >= 1]\n",
    "data = data[data['text'].str.len() >= 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.024310</td>\n",
       "      <td>0.914206</td>\n",
       "      <td>-0.026906</td>\n",
       "      <td>-0.430022</td>\n",
       "      <td>1.614388</td>\n",
       "      <td>-0.341950</td>\n",
       "      <td>1.602143</td>\n",
       "      <td>0.665949</td>\n",
       "      <td>-0.251048</td>\n",
       "      <td>0.953896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.312661</td>\n",
       "      <td>-0.431075</td>\n",
       "      <td>-0.488004</td>\n",
       "      <td>-0.587967</td>\n",
       "      <td>-0.461020</td>\n",
       "      <td>-0.450475</td>\n",
       "      <td>-0.098309</td>\n",
       "      <td>-0.579746</td>\n",
       "      <td>-0.540799</td>\n",
       "      <td>-0.494906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.408779</td>\n",
       "      <td>0.722023</td>\n",
       "      <td>0.126793</td>\n",
       "      <td>-0.272076</td>\n",
       "      <td>0.965823</td>\n",
       "      <td>-0.016373</td>\n",
       "      <td>0.090630</td>\n",
       "      <td>0.428674</td>\n",
       "      <td>-0.202756</td>\n",
       "      <td>0.249073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.167925</td>\n",
       "      <td>0.241565</td>\n",
       "      <td>0.126793</td>\n",
       "      <td>-0.114131</td>\n",
       "      <td>0.317258</td>\n",
       "      <td>0.200678</td>\n",
       "      <td>0.090630</td>\n",
       "      <td>0.784586</td>\n",
       "      <td>0.231872</td>\n",
       "      <td>0.601485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.793248</td>\n",
       "      <td>0.049382</td>\n",
       "      <td>-0.795402</td>\n",
       "      <td>-0.666940</td>\n",
       "      <td>-0.201594</td>\n",
       "      <td>-0.667526</td>\n",
       "      <td>-0.287249</td>\n",
       "      <td>-0.283152</td>\n",
       "      <td>-0.733967</td>\n",
       "      <td>-0.103338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      anger  anticipation   disgust      fear       joy   sadness  surprise  \\\n",
       "0 -0.024310      0.914206 -0.026906 -0.430022  1.614388 -0.341950  1.602143   \n",
       "1 -0.312661     -0.431075 -0.488004 -0.587967 -0.461020 -0.450475 -0.098309   \n",
       "2 -0.408779      0.722023  0.126793 -0.272076  0.965823 -0.016373  0.090630   \n",
       "3  0.167925      0.241565  0.126793 -0.114131  0.317258  0.200678  0.090630   \n",
       "4 -0.793248      0.049382 -0.795402 -0.666940 -0.201594 -0.667526 -0.287249   \n",
       "\n",
       "      trust  negative  positive  \n",
       "0  0.665949 -0.251048  0.953896  \n",
       "1 -0.579746 -0.540799 -0.494906  \n",
       "2  0.428674 -0.202756  0.249073  \n",
       "3  0.784586  0.231872  0.601485  \n",
       "4 -0.283152 -0.733967 -0.103338  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = data.loc[:,['anger','anticipation','disgust','fear','joy','sadness','surprise','trust','negative','positive']]\n",
    "\n",
    "# Zero Score Dataframe Normalization\n",
    "MaxMin_df = stats.zscore(df1)\n",
    "MaxMin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the new dataset which we will be working with\n",
    "df2 = data.loc[:,['title','text','type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether any values are missing\n",
    "# df2.isnull().sum()\n",
    "# df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging columns\n",
    "# df2['content'] = df2['title'] + df2['text']\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "fake    11731\n",
      "real     8074\n",
      "Name: title, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAESCAYAAAD9gqKNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAS2UlEQVR4nO3dfbCcZ1nH8e+PREoK9I2edmoSSYAMkFYEcuxEQQWKNghDOgwdwgANUMlYi4DDiIk60/GPaBkd0SLNEHlpCpUSC9LwUiAGEWHSllPoENISGwi2x9bmyJv1hULq5R97H9mebNJkNz17mv1+Znb22eu57+dcO5PM7zz3s/ucVBWSJD1q2A1IkuYGA0GSBBgIkqTGQJAkAQaCJKkxECRJAMwfdgP9Ov3002vJkiXDbkOSHlFuueWWf6+qsV77HrGBsGTJEiYmJobdhiQ9oiT5l0Pte8gloyTvS7I/yde7an+a5BtJvpbk75Kc0rVvQ5K9SfYkOb+rviLJrrbviiRp9ROSfLjVb0qypN83Kknq35FcQ7gKWDWjth04p6qeAfwzsAEgyXJgDXB2m3NlknltziZgHbCsPaaPeTHwvap6CvAO4O39vhlJUv8eMhCq6gvAd2fUPltVB9rLG4FFbXs1cG1V3V9V+4C9wLlJzgJOqqqd1blXxtXABV1ztrTt64Dzps8eJEmz51h8yuj1wA1teyFwV9e+yVZb2LZn1h80p4XMD4AnHIO+JElHYaBASPIHwAHgmulSj2F1mPrh5vT6eeuSTCSZmJqaOtp2JUmH0XcgJFkLvAR4Vf3klqmTwOKuYYuAu1t9UY/6g+YkmQ+czIwlqmlVtbmqxqtqfGys56emJEl96isQkqwCfg94aVX9d9eubcCa9smhpXQuHt9cVfcA9yVZ2a4PXARc3zVnbdt+OfC58p7ckjTrHvJ7CEk+BDwPOD3JJHAZnU8VnQBsb9d/b6yq36yq3Um2ArfRWUq6tKoeaIe6hM4nlhbQueYwfd3hvcAHkuylc2aw5ti8NUnS0cgj9Zfx8fHxeiR8MW3J+k8Ou4Xjyrcvf/GwW5Ae0ZLcUlXjvfZ5LyNJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJKahwyEJO9Lsj/J17tqpyXZnuSO9nxq174NSfYm2ZPk/K76iiS72r4rkqTVT0jy4Va/KcmSY/weJUlH4EjOEK4CVs2orQd2VNUyYEd7TZLlwBrg7DbnyiTz2pxNwDpgWXtMH/Ni4HtV9RTgHcDb+30zkqT+PWQgVNUXgO/OKK8GtrTtLcAFXfVrq+r+qtoH7AXOTXIWcFJV7ayqAq6eMWf6WNcB502fPUiSZk+/1xDOrKp7ANrzGa2+ELira9xkqy1s2zPrD5pTVQeAHwBP6LMvSVKfjvVF5V6/2ddh6oebc/DBk3VJJpJMTE1N9dmiJKmXfgPh3rYMRHve3+qTwOKucYuAu1t9UY/6g+YkmQ+czMFLVABU1eaqGq+q8bGxsT5blyT10m8gbAPWtu21wPVd9TXtk0NL6Vw8vrktK92XZGW7PnDRjDnTx3o58Ll2nUGSNIvmP9SAJB8CngecnmQSuAy4HNia5GLgTuBCgKranWQrcBtwALi0qh5oh7qEzieWFgA3tAfAe4EPJNlL58xgzTF5Z5Kko/KQgVBVrzzErvMOMX4jsLFHfQI4p0f9h7RAkSQNj99UliQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqRkoEJL8TpLdSb6e5ENJHpPktCTbk9zRnk/tGr8hyd4ke5Kc31VfkWRX23dFkgzSlyTp6PUdCEkWAm8CxqvqHGAesAZYD+yoqmXAjvaaJMvb/rOBVcCVSea1w20C1gHL2mNVv31Jkvoz6JLRfGBBkvnAicDdwGpgS9u/Bbigba8Grq2q+6tqH7AXODfJWcBJVbWzqgq4umuOJGmWzO93YlX9a5I/A+4E/gf4bFV9NsmZVXVPG3NPkjPalIXAjV2HmGy1H7ftmXVJD6Ml6z857BaOK9++/MXDbmFggywZnUrnt/6lwE8Dj03y6sNN6VGrw9R7/cx1SSaSTExNTR1ty5KkwxhkyeiFwL6qmqqqHwMfBX4RuLctA9Ge97fxk8DirvmL6CwxTbbtmfWDVNXmqhqvqvGxsbEBWpckzTRIINwJrExyYvtU0HnA7cA2YG0bsxa4vm1vA9YkOSHJUjoXj29uy0v3JVnZjnNR1xxJ0iwZ5BrCTUmuA74CHAC+CmwGHgdsTXIxndC4sI3fnWQrcFsbf2lVPdAOdwlwFbAAuKE9JEmzqO9AAKiqy4DLZpTvp3O20Gv8RmBjj/oEcM4gvUiSBuM3lSVJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSMGAgJDklyXVJvpHk9iS/kOS0JNuT3NGeT+0avyHJ3iR7kpzfVV+RZFfbd0WSDNKXJOnoDXqG8JfAp6vqacDPAbcD64EdVbUM2NFek2Q5sAY4G1gFXJlkXjvOJmAdsKw9Vg3YlyTpKPUdCElOAn4ZeC9AVf2oqr4PrAa2tGFbgAva9mrg2qq6v6r2AXuBc5OcBZxUVTurqoCru+ZIkmbJIGcITwKmgPcn+WqS9yR5LHBmVd0D0J7PaOMXAnd1zZ9stYVte2ZdkjSLBgmE+cCzgU1V9Szgv2jLQ4fQ67pAHaZ+8AGSdUkmkkxMTU0dbb+SpMMYJBAmgcmquqm9vo5OQNzbloFoz/u7xi/umr8IuLvVF/WoH6SqNlfVeFWNj42NDdC6JGmmvgOhqv4NuCvJU1vpPOA2YBuwttXWAte37W3AmiQnJFlK5+LxzW1Z6b4kK9uniy7qmiNJmiXzB5z/28A1SR4NfAt4HZ2Q2ZrkYuBO4EKAqtqdZCud0DgAXFpVD7TjXAJcBSwAbmgPSdIsGigQqupWYLzHrvMOMX4jsLFHfQI4Z5BeJEmD8ZvKkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQMHQpJ5Sb6a5BPt9WlJtie5oz2f2jV2Q5K9SfYkOb+rviLJrrbviiQZtC9J0tE5FmcIbwZu73q9HthRVcuAHe01SZYDa4CzgVXAlUnmtTmbgHXAsvZYdQz6kiQdhYECIcki4MXAe7rKq4EtbXsLcEFX/dqqur+q9gF7gXOTnAWcVFU7q6qAq7vmSJJmyaBnCH8BvA34367amVV1D0B7PqPVFwJ3dY2bbLWFbXtmXZI0i/oOhCQvAfZX1S1HOqVHrQ5T7/Uz1yWZSDIxNTV1hD9WknQkBjlDeA7w0iTfBq4FXpDkg8C9bRmI9ry/jZ8EFnfNXwTc3eqLetQPUlWbq2q8qsbHxsYGaF2SNFPfgVBVG6pqUVUtoXOx+HNV9WpgG7C2DVsLXN+2twFrkpyQZCmdi8c3t2Wl+5KsbJ8uuqhrjiRplsx/GI55ObA1ycXAncCFAFW1O8lW4DbgAHBpVT3Q5lwCXAUsAG5oD0nSLDomgVBVnwc+37a/A5x3iHEbgY096hPAOceiF0lSf/ymsiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSU3fgZBkcZJ/SHJ7kt1J3tzqpyXZnuSO9nxq15wNSfYm2ZPk/K76iiS72r4rkmSwtyVJOlqDnCEcAN5aVU8HVgKXJlkOrAd2VNUyYEd7Tdu3BjgbWAVcmWReO9YmYB2wrD1WDdCXJKkPfQdCVd1TVV9p2/cBtwMLgdXAljZsC3BB214NXFtV91fVPmAvcG6Ss4CTqmpnVRVwddccSdIsOSbXEJIsAZ4F3AScWVX3QCc0gDPasIXAXV3TJlttYdueWZckzaKBAyHJ44CPAG+pqv843NAetTpMvdfPWpdkIsnE1NTU0TcrSTqkgQIhyU/RCYNrquqjrXxvWwaiPe9v9Ulgcdf0RcDdrb6oR/0gVbW5qsaranxsbGyQ1iVJMwzyKaMA7wVur6o/79q1DVjbttcC13fV1yQ5IclSOhePb27LSvclWdmOeVHXHEnSLJk/wNznAK8BdiW5tdV+H7gc2JrkYuBO4EKAqtqdZCtwG51PKF1aVQ+0eZcAVwELgBvaQ5I0i/oOhKr6Ir3X/wHOO8ScjcDGHvUJ4Jx+e5EkDc5vKkuSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkYA4FQpJVSfYk2Ztk/bD7kaRRMycCIck84F3Ai4DlwCuTLB9uV5I0WuZEIADnAnur6ltV9SPgWmD1kHuSpJEyVwJhIXBX1+vJVpMkzZL5w26gSY9aHTQoWQesay//M8meh7Wr0XI68O/DbuKh5O3D7kBD4L/NY+uJh9oxVwJhEljc9XoRcPfMQVW1Gdg8W02NkiQTVTU+7D6kmfy3OXvmypLRl4FlSZYmeTSwBtg25J4kaaTMiTOEqjqQ5I3AZ4B5wPuqaveQ25KkkTInAgGgqj4FfGrYfYwwl+I0V/lvc5ak6qBrt5KkETRXriFIkobMQJAkAQaCJKmZMxeVNbuSnAi8FfiZqnpDkmXAU6vqE0NuTSMsyWmH219V352tXkaRF5VHVJIPA7cAF1XVOUkWADur6pnD7UyjLMk+Oncp6Hn3gqp60iy3NFI8QxhdT66qVyR5JUBV/U+SXv8JpVlTVUuH3cMoMxBG14/aWUEBJHkycP9wW5J+IsmpwDLgMdO1qvrC8Do6/hkIo+sy4NPA4iTXAM8BXjvUjqQmyW8Ab6ZzX7NbgZXATuAFQ2zruOc1hBHVLt6Fzn+0ADcCj6+qfUNtTAKS7AJ+Hrixqp6Z5GnAH1XVK4bc2nHNj52Oro8DP66qT7ZPFo21mjQX/LCqfgiQ5ISq+gbw1CH3dNxzyWh0/THw8SS/DjwNuBp41XBbkv7fZJJTgI8B25N8jx63xNex5ZLRCEtyAfA24PHAy6rqjuF2JB0sya8AJwOfbn9iVw8TA2HEJHknD/5rdC8AvgV8G6Cq3jSEtqSDJHkusKyq3p9kDHic17geXi4ZjZ6JGa9vGUoX0mEkuQwYp3Pd4P3ATwEfpPNpOD1MPEOQNOckuRV4FvCVqnpWq32tqp4x1MaOc54hjKh276I/AZbz4C/+eGsAzQU/qqpKMv3FyccOu6FR4MdOR9f7gU3AAeD5dD5l9IGhdiQB7RYqn0jybuCUJG8A/h746+F2dvxzyWhEJbmlqlYk2VVVP9tq/1RVvzTs3qQkXwF+D/g1Ol+c/ExVbR9uV8c/l4xG1w+TPAq4I8kbgX8FzhhyT9K0ncD3q+p3h93IKHHJaMQkmV4Wuh44EXgTsAJ4DbB2WH1JMzwf2Jnkm0m+Nv0YdlPHO5eMRkyS24AXAduA5zHjvvP+ARLNBUme2KteVf8y272MEgNhxCR5E3AJ8CQ6y0ThJ3+QxD9AIo0wA2FEJdlUVZcMuw9Jc4eBIEkCvKgsSWoMBEkSYCBIRyzJKUl+a9h9SA8XA0E6cqcABoKOWwaCdOQuB56c5NYkf5tk9fSOJNckeWmS1ya5Psmnk+xpt3GeHvPqJDe3+e9OMm8o70I6BANBOnLrgW9W1TOBvwJeB5DkZOAXgU+1cefS+XOkzwQuTDKe5OnAK4DntPkP4J8s1RzjvYykPlTVPyZ5V5IzgJcBH6mqA50bdbK9qr4DkOSjwHPp3FV2BfDlNmYBsH8ozUuHYCBI/fsAnd/y1wCv76rP/HLP9DfBt1TVhlnqTTpqLhlJR+4+4PFdr68C3gJQVbu76r+a5LQkC4ALgC8BO4CXtzMK2v6e9+uRhsUzBOkIVdV3knwpydeBG6rqd5PcDnxsxtAv0jl7eArwN1U1AZDkD4HPttuO/xi4FPBmbZozvHWF1KckJwK7gGdX1Q9a7bXAeFW9cZi9Sf1wyUjqQ5IXAt8A3jkdBtIjnWcIkiTAMwRJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAmA/wPlTiP8vN8aGQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How many fake and real articles?\n",
    "print(df2.groupby(['type'])['title'].count())\n",
    "df2.groupby(['type'])['title'].count().plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[At, Donald, Trump, Properties, Showcase, Bran...</td>\n",
       "      <td>[They, stood, line, Trump, Tower, sometimes, h...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Trump, Foundation, Tells, New, York, It, Has,...</td>\n",
       "      <td>[Donald, J, Trump, foundation, informed, Attor...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Donald, Trump, Prepares, White, House, Move, ...</td>\n",
       "      <td>[President, Donald, J, Trump, White, House, ou...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Luring, Chinese, Investors, With, Trump, Name...</td>\n",
       "      <td>[An, investment, pitch, new, Texas, hotel, try...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Melania, Barron, Trump, Won, Immediately, Mov...</td>\n",
       "      <td>[President, Donald, J, Trump, wife, Melania, s...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19800</th>\n",
       "      <td>[Alabama, Lawmaker, Same, Couples, Don, Deserv...</td>\n",
       "      <td>[Most, conservatives, oppose, marriage, equali...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19801</th>\n",
       "      <td>[GOP, Senator, David, Perdue, Jokes, About, Pr...</td>\n",
       "      <td>[The, freshman, senator, Georgia, quoted, scri...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19802</th>\n",
       "      <td>[State, Department, says, find, emails, Clinto...</td>\n",
       "      <td>[The, State, Department, told, Republican, Nat...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19803</th>\n",
       "      <td>[In, Ethiopia, Obama, seeks, progress, peace, ...</td>\n",
       "      <td>[ADDIS, ABABA, Ethiopia, Obama, convened, meet...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19804</th>\n",
       "      <td>[Jeb, Bush, Is, Suddenly, Attacking, Trump, He...</td>\n",
       "      <td>[Jeb, Bush, Is, Suddenly, Attacking, Trump, He...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19805 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      [At, Donald, Trump, Properties, Showcase, Bran...   \n",
       "1      [Trump, Foundation, Tells, New, York, It, Has,...   \n",
       "2      [Donald, Trump, Prepares, White, House, Move, ...   \n",
       "3      [Luring, Chinese, Investors, With, Trump, Name...   \n",
       "4      [Melania, Barron, Trump, Won, Immediately, Mov...   \n",
       "...                                                  ...   \n",
       "19800  [Alabama, Lawmaker, Same, Couples, Don, Deserv...   \n",
       "19801  [GOP, Senator, David, Perdue, Jokes, About, Pr...   \n",
       "19802  [State, Department, says, find, emails, Clinto...   \n",
       "19803  [In, Ethiopia, Obama, seeks, progress, peace, ...   \n",
       "19804  [Jeb, Bush, Is, Suddenly, Attacking, Trump, He...   \n",
       "\n",
       "                                                    text  type  \n",
       "0      [They, stood, line, Trump, Tower, sometimes, h...  real  \n",
       "1      [Donald, J, Trump, foundation, informed, Attor...  real  \n",
       "2      [President, Donald, J, Trump, White, House, ou...  real  \n",
       "3      [An, investment, pitch, new, Texas, hotel, try...  real  \n",
       "4      [President, Donald, J, Trump, wife, Melania, s...  real  \n",
       "...                                                  ...   ...  \n",
       "19800  [Most, conservatives, oppose, marriage, equali...  real  \n",
       "19801  [The, freshman, senator, Georgia, quoted, scri...  real  \n",
       "19802  [The, State, Department, told, Republican, Nat...  real  \n",
       "19803  [ADDIS, ABABA, Ethiopia, Obama, convened, meet...  real  \n",
       "19804  [Jeb, Bush, Is, Suddenly, Attacking, Trump, He...  real  \n",
       "\n",
       "[19805 rows x 3 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2[df2['text'].map(len) >= 1]\n",
    "#Reset index\n",
    "df2 = df2.reset_index().drop(\"index\", axis=1)\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Matrix\n",
    "embed_mat = np.zeros((50000,300))\n",
    "for i, e in model.key_to_index.items():\n",
    "    # embed_mat[model.key_to_index['e']] = model[i]\n",
    "    embed_mat[model.get_index('e')] = model[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying it to the columns (Topic modeling and document similarity analysis)\n",
    "\n",
    "df2['title'] = df2.apply(lambda r: [model.key_to_index[x] for x in r['title'] if x in model.key_to_index], axis=1)\n",
    "df2['text'] = df2.apply(lambda r: [model.key_to_index[x] for x in r['text'] if x in model.key_to_index], axis=1)\n",
    "# df2['content'] = df2.apply(lambda r: [model.key_to_index[x] for x in r['content'] if x in model.key_to_index], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "encoder = LabelBinarizer().fit(list(df2['type']))\n",
    "df2['type'] = df2.swifter.apply(lambda r: encoder.transform([r['type']])[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_array(array, token_len):\n",
    "    diff_token = token_len - len(array)\n",
    "    if(diff_token < 0):\n",
    "        array = array[:token_len] #Truncate\n",
    "    else:\n",
    "        #Pad\n",
    "        array += [0]*diff_token \n",
    "        \n",
    "    return array \n",
    "\n",
    "# tokens_numbers_text = df2.swifter.apply(lambda row: len(row['text']), axis = 1)\n",
    "# max_tokens_text = int(np.mean(tokens_numbers_text) + 2 * np.std(tokens_numbers_text))\n",
    "\n",
    "# tokens_numbers_title = df2.swifter.apply(lambda row: len(row['title']), axis = 1)\n",
    "# max_tokens_title = int(np.mean(tokens_numbers_title) + 2 * np.std(tokens_numbers_title))\n",
    "# print(max_tokens_text,max_tokens_title)    \n",
    "\n",
    "# Check the highest length of text and title:\n",
    "max_tokens_text = 13000\n",
    "max_tokens_title = 48\n",
    "\n",
    "df3 = df2.loc[:,['title','text','type']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['text'] = df2.swifter.apply(lambda r: pad_array(r['text'], max_tokens_text) , axis=1)\n",
    "# df3['title'] = df2.swifter.apply(lambda r: pad_array(r['title'], max_tokens_title) , axis=1)\n",
    "\n",
    "\n",
    "df3['text'] = list(pad_sequences((df2['text']), maxlen=max_tokens_text,padding='post'))\n",
    "df3['title'] = list(pad_sequences(df2['title'], maxlen=max_tokens_title,padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train - test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df3[['text', 'title']], df3['type'], test_size=0.2, random_state=42)\n",
    "\n",
    "#Train - valid\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42)\n",
    " \n",
    "\n",
    "train_fit = [np.asarray(X_train['title'].tolist()), \n",
    "             np.asarray(X_train['text'].tolist())]\n",
    "\n",
    "valid_fit = [np.asarray(X_valid['title'].tolist()), \n",
    "             np.asarray(X_valid['text'].tolist())]\n",
    "\n",
    "test_fit = [np.asarray(X_test['title'].tolist()), \n",
    "             np.asarray(X_test['text']  .tolist())]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_input (InputLayer)        [(None, 48)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "text_input (InputLayer)         [(None, 13000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embed_title (Embedding)         (None, 48, 300)      15000000    title_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embed_text (Embedding)          (None, 13000, 300)   15000000    text_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv1Title (Conv1D)             (None, 23, 5)        6005        embed_title[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2Text (Conv1D)              (None, 3247, 80)     384080      embed_text[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Pool1Title (MaxPooling1D)       (None, 11, 5)        0           Conv1Title[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Pool2Text (MaxPooling1D)        (None, 811, 80)      0           Conv2Text[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 55)           0           Pool1Title[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 64880)        0           Pool2Text[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dense1Title (Dense)             (None, 50)           2800        flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dense1Text (Dense)              (None, 100)          6488100     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 150)          0           Dense1Title[0][0]                \n",
      "                                                                 Dense1Text[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 50)           7550        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 50)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 50)           2550        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 50)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            51          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 36,891,136\n",
      "Trainable params: 6,891,136\n",
      "Non-trainable params: 30,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_title\n",
    "title_input = k.layers.Input(shape=(max_tokens_title,), name='title_input')\n",
    "inp = k.layers.Embedding(output_dim=300, input_dim=50000, trainable=False, name='embed_title')(title_input)\n",
    "\n",
    "#Added\n",
    "x = k.layers.Conv1D(filters = 5, kernel_size=4, strides=2, activation='relu', name='Conv1Title')(inp)\n",
    "x = k.layers.MaxPool1D(pool_size = 2, name='Pool1Title')(x)\n",
    "x = k.layers.Flatten()(x)\n",
    "x = k.layers.Dense(50, activation='relu', name='Dense1Title', kernel_regularizer='l2')(x)\n",
    "\n",
    "#input_Title\n",
    "text_input = k.layers.Input(shape=(max_tokens_text,), name='text_input')\n",
    "inp2 = k.layers.Embedding(output_dim=300, input_dim=50000, trainable=False, name='embed_text')(text_input)\n",
    "x2 = k.layers.Conv1D(filters = 40, kernel_size = 16, strides = 2, activation='relu', name='Conv1Text')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4, name='Pool1Text')(x2)\n",
    "\n",
    "#Added\n",
    "x2 = k.layers.Conv1D(filters = 80, kernel_size = 16, strides = 4, activation='relu', name='Conv2Text')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4, name='Pool2Text')(x2)\n",
    "x2 = k.layers.Flatten()(x2)\n",
    "x2 = k.layers.Dense(100, activation='relu', kernel_regularizer='l2', name='Dense1Text')(x2)\n",
    "\n",
    "\n",
    "#Merge\n",
    "x = k.layers.concatenate([x, x2])\n",
    "\n",
    "#Common part\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "out = k.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "#Build model\n",
    "model = k.models.Model(inputs=[title_input, text_input], outputs=[out])\n",
    "model.compile(k.optimizers.RMSprop(), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[59,2] = 75042 is not in [0, 50000)\n\t [[node model_2/embed_title/embedding_lookup (defined at tmp/ipykernel_18119/2540345468.py:1) ]] [Op:__inference_train_function_2380]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_2/embed_title/embedding_lookup:\n model_2/embed_title/embedding_lookup/1778 (defined at home/users/s/sarthak_7034/miniconda3/lib/python3.9/contextlib.py:119)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18119/2540345468.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m hist = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=128, epochs=20,\n\u001b[0m\u001b[1;32m      2\u001b[0m         callbacks = [k.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta = 0.1)], validation_data=(valid_fit, np.array(y_valid.tolist())))\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[59,2] = 75042 is not in [0, 50000)\n\t [[node model_2/embed_title/embedding_lookup (defined at tmp/ipykernel_18119/2540345468.py:1) ]] [Op:__inference_train_function_2380]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_2/embed_title/embedding_lookup:\n model_2/embed_title/embedding_lookup/1778 (defined at home/users/s/sarthak_7034/miniconda3/lib/python3.9/contextlib.py:119)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=128, epochs=20,\n",
    "        callbacks = [k.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta = 0.1)], validation_data=(valid_fit, np.array(y_valid.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion Matrix of SVM\n",
    "# cm = metrics.confusion_matrix(y_test, prediction)\n",
    "# plot_confusion_matrix(cm, classes=['Fake', 'Real'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c79634922abe25a0b9d55bc2eb6abac259bd6c8bbb4022582053c5e5bdfcda26"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
