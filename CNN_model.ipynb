{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7042"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comet_ml at the top of your file\n",
    "# from comet_ml import Experiment\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# from matplotlib import style\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import swifter\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras as k\n",
    "import tensorflow as tf\n",
    "# import torch\n",
    "from scipy import stats\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "# Create an experiment with your api key\n",
    "# experiment = Experiment(\n",
    "#     api_key=\"BKac2uRt0FMAlheXf6HClaZhD\",\n",
    "#     project_name=\"general\",\n",
    "#     workspace=\"sarthak7034\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/s/sarthak_7034/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 12:31:34.048778: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('gpu')))\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to plot the confusion matrix (code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "    \n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, cm[i, j],\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "data = pd.read_csv(\"./all_data.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = stop_words.remove('not')\n",
    "def clean_text(text):\n",
    "    #Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    #Tokenize\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # remove stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]     # Experiment with removing it!!!\n",
    "    #Remove non alphanumerica characters\n",
    "    words = [word for word in tokens if word.isalpha()]    \n",
    "    return words\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd5e9c4fbaa4f8d8725138feaeee12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/20015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e597f6c47b4ac09da0084988dde1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/20015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying lamdba function to clean text\n",
    "data['text'] = data.swifter.apply(lambda row: clean_text(row['text']), axis=1)\n",
    "#Clean title\n",
    "data['title'] = data.swifter.apply(lambda row: clean_text(row['title']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing row with only text\n",
    "data = data[data['title'].str.len() >= 1]\n",
    "data = data[data['text'].str.len() >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = data.loc[:,['anger','anticipation','disgust','fear','joy','sadness','surprise','trust','negative','positive']]\n",
    "\n",
    "# # Zero Score Dataframe Normalization\n",
    "# MaxMin_df = stats.zscore(df1)\n",
    "# MaxMin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the new dataset which we will be working with\n",
    "df2 = data.loc[:,['title','text','type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether any values are missing\n",
    "# df2.isnull().sum()\n",
    "# df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[At, Donald, Trump, Properties, Showcase, Bran...</td>\n",
       "      <td>[They, stood, line, Trump, Tower, sometimes, h...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Trump, Foundation, Tells, New, York, It, Has,...</td>\n",
       "      <td>[Donald, J, Trump, foundation, informed, Attor...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Donald, Trump, Prepares, White, House, Move, ...</td>\n",
       "      <td>[President, Donald, J, Trump, White, House, ou...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Luring, Chinese, Investors, With, Trump, Name...</td>\n",
       "      <td>[An, investment, pitch, new, Texas, hotel, try...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Melania, Barron, Trump, Won, Immediately, Mov...</td>\n",
       "      <td>[President, Donald, J, Trump, wife, Melania, s...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19800</th>\n",
       "      <td>[Alabama, Lawmaker, Same, Couples, Don, Deserv...</td>\n",
       "      <td>[Most, conservatives, oppose, marriage, equali...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19801</th>\n",
       "      <td>[GOP, Senator, David, Perdue, Jokes, About, Pr...</td>\n",
       "      <td>[The, freshman, senator, Georgia, quoted, scri...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19802</th>\n",
       "      <td>[State, Department, says, find, emails, Clinto...</td>\n",
       "      <td>[The, State, Department, told, Republican, Nat...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19803</th>\n",
       "      <td>[In, Ethiopia, Obama, seeks, progress, peace, ...</td>\n",
       "      <td>[ADDIS, ABABA, Ethiopia, Obama, convened, meet...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19804</th>\n",
       "      <td>[Jeb, Bush, Is, Suddenly, Attacking, Trump, He...</td>\n",
       "      <td>[Jeb, Bush, Is, Suddenly, Attacking, Trump, He...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19805 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      [At, Donald, Trump, Properties, Showcase, Bran...   \n",
       "1      [Trump, Foundation, Tells, New, York, It, Has,...   \n",
       "2      [Donald, Trump, Prepares, White, House, Move, ...   \n",
       "3      [Luring, Chinese, Investors, With, Trump, Name...   \n",
       "4      [Melania, Barron, Trump, Won, Immediately, Mov...   \n",
       "...                                                  ...   \n",
       "19800  [Alabama, Lawmaker, Same, Couples, Don, Deserv...   \n",
       "19801  [GOP, Senator, David, Perdue, Jokes, About, Pr...   \n",
       "19802  [State, Department, says, find, emails, Clinto...   \n",
       "19803  [In, Ethiopia, Obama, seeks, progress, peace, ...   \n",
       "19804  [Jeb, Bush, Is, Suddenly, Attacking, Trump, He...   \n",
       "\n",
       "                                                    text  type  \n",
       "0      [They, stood, line, Trump, Tower, sometimes, h...  real  \n",
       "1      [Donald, J, Trump, foundation, informed, Attor...  real  \n",
       "2      [President, Donald, J, Trump, White, House, ou...  real  \n",
       "3      [An, investment, pitch, new, Texas, hotel, try...  real  \n",
       "4      [President, Donald, J, Trump, wife, Melania, s...  real  \n",
       "...                                                  ...   ...  \n",
       "19800  [Most, conservatives, oppose, marriage, equali...  real  \n",
       "19801  [The, freshman, senator, Georgia, quoted, scri...  real  \n",
       "19802  [The, State, Department, told, Republican, Nat...  real  \n",
       "19803  [ADDIS, ABABA, Ethiopia, Obama, convened, meet...  real  \n",
       "19804  [Jeb, Bush, Is, Suddenly, Attacking, Trump, He...  real  \n",
       "\n",
       "[19805 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2[df2['text'].map(len) >= 1]\n",
    "#Reset index\n",
    "df2 = df2.reset_index().drop(\"index\", axis=1)\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "fake    11731\n",
      "real     8074\n",
      "Name: title, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How many fake and real articles?\n",
    "print(df2.groupby(['type'])['title'].count())\n",
    "df2.groupby(['type'])['title'].count().plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed = KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(model_embed.key_to_index.keys())\n",
    "print(vocab_length)                                         #Input dimension size(3000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fbe7a2af60440890a78cbc178f0072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9c7bc68b2b4863a9010398a7666fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying it to the columns \n",
    "\n",
    "df2['title'] = df2.swifter.apply(lambda r: [model_embed.key_to_index[x] for x in r['title'] if x in model_embed.key_to_index], axis=1)\n",
    "df2['text'] = df2.swifter.apply(lambda r: [model_embed.key_to_index[x] for x in r['text'] if x in model_embed.key_to_index], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-hot encoding\n",
    "# encoder = LabelBinarizer().fit(list(df2['type']))\n",
    "# df2['type'] = df2.swifter.apply(lambda r: encoder.transform([r['type']])[0], axis=1)\n",
    "\n",
    "variable_name = {'fake' : 0 , 'real' : 1 }\n",
    "df2['type'] = df2['type'].map(variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[332, 6117, 13034, 11062, 20257, 9193, 446]</td>\n",
       "      <td>[128, 2379, 318, 13034, 9413, 1631, 242, 957, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[13034, 2124, 44625, 442, 5353, 51, 7754, 8808...</td>\n",
       "      <td>[6117, 4883, 13034, 3113, 2826, 4605, 1927, 33...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6117, 13034, 49031, 1790, 659, 11977, 553, 94...</td>\n",
       "      <td>[446, 6117, 4883, 13034, 1790, 659, 16780, 177...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[176192, 1035, 3789, 316, 13034, 12441, 3337, ...</td>\n",
       "      <td>[741, 615, 1924, 65, 675, 1639, 469, 8793, 112...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[282726, 14896, 13034, 26994, 17785, 11977, 17...</td>\n",
       "      <td>[446, 6117, 4883, 13034, 783, 282726, 831, 148...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19667</th>\n",
       "      <td>[2625, 47190, 14968, 22328, 4119, 115896, 1496...</td>\n",
       "      <td>[1214, 6555, 5472, 2724, 10729, 12687, 6484, 3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19668</th>\n",
       "      <td>[2751, 4301, 689, 19370, 82238, 1523, 94624, 4...</td>\n",
       "      <td>[7, 2779, 4264, 1742, 2717, 38413, 168, 1956, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19669</th>\n",
       "      <td>[268, 423, 115, 359, 9451, 1820, 1735, 3803]</td>\n",
       "      <td>[7, 268, 423, 162, 917, 430, 1468, 75, 359, 94...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19670</th>\n",
       "      <td>[70, 9711, 494, 3786, 1463, 1356, 371, 1063, 1...</td>\n",
       "      <td>[1538939, 1151610, 9711, 494, 13595, 349, 618,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19671</th>\n",
       "      <td>[69891, 513, 1732, 13741, 59535, 13034, 2165, ...</td>\n",
       "      <td>[69891, 513, 1732, 13741, 59535, 13034, 2165, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19672 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0            [332, 6117, 13034, 11062, 20257, 9193, 446]   \n",
       "1      [13034, 2124, 44625, 442, 5353, 51, 7754, 8808...   \n",
       "2      [6117, 13034, 49031, 1790, 659, 11977, 553, 94...   \n",
       "3      [176192, 1035, 3789, 316, 13034, 12441, 3337, ...   \n",
       "4      [282726, 14896, 13034, 26994, 17785, 11977, 17...   \n",
       "...                                                  ...   \n",
       "19667  [2625, 47190, 14968, 22328, 4119, 115896, 1496...   \n",
       "19668  [2751, 4301, 689, 19370, 82238, 1523, 94624, 4...   \n",
       "19669       [268, 423, 115, 359, 9451, 1820, 1735, 3803]   \n",
       "19670  [70, 9711, 494, 3786, 1463, 1356, 371, 1063, 1...   \n",
       "19671  [69891, 513, 1732, 13741, 59535, 13034, 2165, ...   \n",
       "\n",
       "                                                    text  type  \n",
       "0      [128, 2379, 318, 13034, 9413, 1631, 242, 957, ...     1  \n",
       "1      [6117, 4883, 13034, 3113, 2826, 4605, 1927, 33...     1  \n",
       "2      [446, 6117, 4883, 13034, 1790, 659, 16780, 177...     1  \n",
       "3      [741, 615, 1924, 65, 675, 1639, 469, 8793, 112...     1  \n",
       "4      [446, 6117, 4883, 13034, 783, 282726, 831, 148...     1  \n",
       "...                                                  ...   ...  \n",
       "19667  [1214, 6555, 5472, 2724, 10729, 12687, 6484, 3...     1  \n",
       "19668  [7, 2779, 4264, 1742, 2717, 38413, 168, 1956, ...     1  \n",
       "19669  [7, 268, 423, 162, 917, 430, 1468, 75, 359, 94...     1  \n",
       "19670  [1538939, 1151610, 9711, 494, 13595, 349, 618,...     1  \n",
       "19671  [69891, 513, 1732, 13741, 59535, 13034, 2165, ...     1  \n",
       "\n",
       "[19672 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2[df2['text'].map(len) >= 1]\n",
    "#Reset index\n",
    "df2 = df2.reset_index().drop(\"index\", axis=1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting all to a new dataframe\n",
    "df3 = df2.loc[:,['title','text','type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the highest length of text and title:\n",
    "max_tokens_text = 13000\n",
    "max_tokens_title = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['text'] = list(pad_sequences((df2['text']), maxlen=max_tokens_text,padding='post'))\n",
    "df3['title'] = list(pad_sequences(df2['title'], maxlen=max_tokens_title,padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python object serialized\n",
    "df3.to_pickle('./model_data.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pickle = pd.read_pickle('./model_data.pickle')\n",
    "data_pickle = data_pickle.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train - test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_pickle[['text', 'title']], data_pickle['type'], test_size=0.2, random_state=42)\n",
    "\n",
    "#Train - valid\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42)\n",
    " \n",
    "\n",
    "train_fit = [np.asarray(X_train['title'].tolist()), \n",
    "             np.asarray(X_train['text'].tolist())\n",
    "             ]\n",
    "\n",
    "valid_fit = [np.asarray(X_valid['title'].tolist()), \n",
    "             np.asarray(X_valid['text'].tolist())\n",
    "             ]\n",
    "\n",
    "test_fit = [np.asarray(X_test['title'].tolist()), \n",
    "             np.asarray(X_test['text'].tolist())\n",
    "             ]    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "seed(50)\n",
    "tf.random.set_seed(50)\n",
    "k.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 12:34:53.457829: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-07 12:34:53.486047: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 48)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 13000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 48, 300)      900000000   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 13000, 300)   900000000   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 23, 5)        6005        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 3247, 80)     384080      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Pool1Title (MaxPooling1D)       (None, 11, 5)        0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 811, 80)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 55)           0           Pool1Title[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64880)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Dense1Title (Dense)             (None, 50)           2800        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense1Text (Dense)              (None, 100)          6488100     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 150)          0           Dense1Title[0][0]                \n",
      "                                                                 Dense1Text[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50)           7550        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 50)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           2550        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            51          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,806,891,136\n",
      "Trainable params: 6,891,136\n",
      "Non-trainable params: 1,800,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_title\n",
    "title_input = k.layers.Input(shape=(max_tokens_title,))\n",
    "inp1 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(title_input)\n",
    "\n",
    "#Added\n",
    "x1 = k.layers.Conv1D(filters = 5, kernel_size=4, strides=2, activation='relu')(inp1)\n",
    "x1 = k.layers.MaxPool1D(pool_size = 2, name='Pool1Title')(x1)\n",
    "x1 = k.layers.Flatten()(x1)\n",
    "x1 = k.layers.Dense(50, activation='relu', name='Dense1Title', kernel_regularizer='l2')(x1)\n",
    "\n",
    "#input_Text\n",
    "text_input = k.layers.Input(shape=(max_tokens_text,))\n",
    "inp2 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(text_input)\n",
    "\n",
    "x2 = k.layers.Conv1D(filters = 40, kernel_size = 16, strides = 2, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4)(x2)\n",
    "\n",
    "#Added\n",
    "x2 = k.layers.Conv1D(filters = 80, kernel_size = 16, strides = 4, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4)(x2)\n",
    "x2 = k.layers.Flatten()(x2)\n",
    "x2 = k.layers.Dense(100, activation='relu', kernel_regularizer='l2', name='Dense1Text')(x2)\n",
    "\n",
    "\n",
    "#Merge\n",
    "x = k.layers.concatenate([x1, x2])\n",
    "\n",
    "#Common part\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "out = k.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "#Build model\n",
    "model = k.models.Model(inputs=[title_input, text_input], outputs=[out])\n",
    "\n",
    "model.compile(k.optimizers.RMSprop(), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 12:35:00.581643: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-04-07 12:35:00.638737: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "99/99 [==============================] - 213s 2s/step - loss: 1.3123 - acc: 0.6497 - val_loss: 0.6286 - val_acc: 0.7840\n",
      "Epoch 2/20\n",
      "99/99 [==============================] - 197s 2s/step - loss: 0.5163 - acc: 0.8536 - val_loss: 0.4146 - val_acc: 0.8707\n",
      "Epoch 3/20\n",
      "99/99 [==============================] - 199s 2s/step - loss: 0.4398 - acc: 0.8866 - val_loss: 0.4718 - val_acc: 0.8475\n",
      "Epoch 4/20\n",
      "99/99 [==============================] - 199s 2s/step - loss: 0.2648 - acc: 0.9406 - val_loss: 0.3819 - val_acc: 0.8882\n",
      "Epoch 5/20\n",
      "99/99 [==============================] - 199s 2s/step - loss: 0.2597 - acc: 0.9620 - val_loss: 0.3604 - val_acc: 0.8961\n",
      "Epoch 6/20\n",
      "99/99 [==============================] - 198s 2s/step - loss: 0.1512 - acc: 0.9818 - val_loss: 0.4138 - val_acc: 0.8949\n",
      "Epoch 7/20\n",
      "99/99 [==============================] - 196s 2s/step - loss: 0.1600 - acc: 0.9817 - val_loss: 0.3551 - val_acc: 0.9041\n",
      "Epoch 8/20\n",
      "99/99 [==============================] - 184s 2s/step - loss: 0.1234 - acc: 0.9869 - val_loss: 0.4029 - val_acc: 0.9015\n",
      "Epoch 9/20\n",
      "99/99 [==============================] - 183s 2s/step - loss: 0.1524 - acc: 0.9886 - val_loss: 0.4014 - val_acc: 0.9037\n",
      "Epoch 10/20\n",
      "99/99 [==============================] - 183s 2s/step - loss: 0.1019 - acc: 0.9879 - val_loss: 0.5126 - val_acc: 0.8799\n",
      "Epoch 11/20\n",
      "99/99 [==============================] - 184s 2s/step - loss: 0.0918 - acc: 0.9897 - val_loss: 0.4064 - val_acc: 0.9003\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=128, epochs=20,\n",
    "        callbacks = [k.callbacks.EarlyStopping(monitor='val_loss', patience=4)], \n",
    "        validation_data=(valid_fit, np.array(y_valid.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      2354\n",
      "           1       0.88      0.88      0.88      1581\n",
      "\n",
      "    accuracy                           0.91      3935\n",
      "   macro avg       0.90      0.90      0.90      3935\n",
      "weighted avg       0.91      0.91      0.91      3935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(np.array(y_test.tolist()), test_pred.round())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "seed(50)\n",
    "tf.random.set_seed(50)\n",
    "k.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 48)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 13000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 48, 300)      900000000   input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 13000, 300)   900000000   input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 23, 5)        6005        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 3247, 80)     384080      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 11, 5)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 811, 80)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 55)           0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 64880)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 50)           2800        flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 100)          6488100     flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 150)          0           dense_5[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 50)           7550        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 50)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 50)           2550        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 50)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 4)            204         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,806,891,289\n",
      "Trainable params: 6,891,289\n",
      "Non-trainable params: 1,800,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_title\n",
    "title_input = k.layers.Input(shape=(max_tokens_title,))\n",
    "inp1 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(title_input)\n",
    "\n",
    "x = k.layers.Conv1D(filters = 5, kernel_size=4, strides=2, activation='relu')(inp1)\n",
    "x = k.layers.MaxPool1D(pool_size = 2)(x)\n",
    "x = k.layers.Flatten()(x)\n",
    "x = k.layers.Dense(50, activation='relu',  kernel_regularizer='l2')(x)\n",
    "\n",
    "#input_content\n",
    "text_input = k.layers.Input(shape=(max_tokens_text,))\n",
    "inp2 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(text_input)\n",
    "\n",
    "x2 = k.layers.Conv1D(filters = 40, kernel_size = 16, strides = 2, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4 )(x2)\n",
    "\n",
    "#Added\n",
    "x2 = k.layers.Conv1D(filters = 80, kernel_size = 16, strides = 4, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4, )(x2)\n",
    "x2 = k.layers.Flatten()(x2)\n",
    "x2 = k.layers.Dense(100, activation='relu', kernel_regularizer='l2')(x2)\n",
    "\n",
    "\n",
    "#Merge\n",
    "x = k.layers.concatenate([x, x2])\n",
    "\n",
    "#Common part\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "out = k.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "\n",
    "#Build model\n",
    "model = k.models.Model(inputs=[title_input, text_input], outputs=[out])\n",
    "\n",
    "model.compile(k.optimizers.RMSprop(), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "99/99 [==============================] - 196s 2s/step - loss: 2.2311 - acc: 0.5749 - val_loss: 0.6273 - val_acc: 0.8005\n",
      "Epoch 2/30\n",
      "99/99 [==============================] - 189s 2s/step - loss: 0.5854 - acc: 0.8133 - val_loss: 0.4450 - val_acc: 0.8529\n",
      "Epoch 3/30\n",
      "99/99 [==============================] - 185s 2s/step - loss: 0.4715 - acc: 0.8577 - val_loss: 0.4134 - val_acc: 0.8631\n",
      "Epoch 4/30\n",
      "99/99 [==============================] - 188s 2s/step - loss: 0.3112 - acc: 0.9219 - val_loss: 0.3519 - val_acc: 0.8907\n",
      "Epoch 5/30\n",
      "99/99 [==============================] - 183s 2s/step - loss: 0.2213 - acc: 0.9561 - val_loss: 0.3918 - val_acc: 0.8914\n",
      "Epoch 6/30\n",
      "99/99 [==============================] - 185s 2s/step - loss: 0.1774 - acc: 0.9724 - val_loss: 0.3862 - val_acc: 0.8990\n",
      "Epoch 7/30\n",
      "99/99 [==============================] - 184s 2s/step - loss: 0.1466 - acc: 0.9801 - val_loss: 0.4242 - val_acc: 0.8923\n",
      "Epoch 8/30\n",
      "99/99 [==============================] - 188s 2s/step - loss: 0.1500 - acc: 0.9775 - val_loss: 0.4021 - val_acc: 0.8983\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=128, epochs=30,\n",
    "        callbacks = [k.callbacks.EarlyStopping(monitor='val_loss', patience=4)], \n",
    "        validation_data=(valid_fit, np.array(y_valid.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted indices (ValueError: Classification metrics can't handle a mix of binary and continuous-multioutput targets)\n",
    "test_pred = np.argmax(test_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      2354\n",
      "           1       0.88      0.88      0.88      1581\n",
      "\n",
      "    accuracy                           0.90      3935\n",
      "   macro avg       0.90      0.90      0.90      3935\n",
      "weighted avg       0.90      0.90      0.90      3935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(np.array(y_test.tolist()), test_pred.round())\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c79634922abe25a0b9d55bc2eb6abac259bd6c8bbb4022582053c5e5bdfcda26"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
