{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comet_ml at the top of your file\n",
    "# from comet_ml import Experiment\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# from matplotlib import style\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import swifter\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras as k\n",
    "import tensorflow as tf\n",
    "# import torch\n",
    "from scipy import stats\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "# Create an experiment with your api key\n",
    "# experiment = Experiment(\n",
    "#     api_key=\"BKac2uRt0FMAlheXf6HClaZhD\",\n",
    "#     project_name=\"general\",\n",
    "#     workspace=\"sarthak7034\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('gpu')))\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to plot the confusion matrix (code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "    \n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, cm[i, j],\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "data = pd.read_csv(\"./all_data.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = data.loc[:,['title','text','type']]\n",
    "gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    #Tokenize\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # remove stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]     # Experiment with removing it!!!\n",
    "    #Remove non alphanumerica characters\n",
    "    words = [word for word in tokens if word.isalpha()]    \n",
    "    return words\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying lamdba function to clean text\n",
    "data['text'] = data.swifter.apply(lambda row: clean_text(row['text']), axis=1)\n",
    "#Clean title\n",
    "data['title'] = data.swifter.apply(lambda row: clean_text(row['title']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing row with only text\n",
    "data = data[data['title'].str.len() >= 1]\n",
    "data = data[data['text'].str.len() >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = data.loc[:,['anger','anticipation','disgust','fear','joy','sadness','surprise','trust','negative','positive']]\n",
    "\n",
    "# # Zero Score Dataframe Normalization\n",
    "# MaxMin_df = stats.zscore(df1)\n",
    "# MaxMin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the new dataset which we will be working with\n",
    "df2 = data.loc[:,['title','text','type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[df2['text'].map(len) >= 1]\n",
    "#Reset index\n",
    "df2 = df2.reset_index().drop(\"index\", axis=1)\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether any values are missing\n",
    "# df2.isnull().sum()\n",
    "# df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many fake and real articles?\n",
    "print(df2.groupby(['type'])['title'].count())\n",
    "df2.groupby(['type'])['title'].count().plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed = KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(model_embed.key_to_index.keys())\n",
    "print(vocab_length)                                         #Input dimension size(3000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying it to the columns \n",
    "\n",
    "df2['title'] = df2.swifter.apply(lambda r: [model_embed.key_to_index[x] for x in r['title'] if x in model_embed.key_to_index], axis=1)\n",
    "df2['text'] = df2.swifter.apply(lambda r: [model_embed.key_to_index[x] for x in r['text'] if x in model_embed.key_to_index], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-hot encoding\n",
    "# encoder = LabelBinarizer().fit(list(df2['type']))\n",
    "# df2['type'] = df2.swifter.apply(lambda r: encoder.transform([r['type']])[0], axis=1)\n",
    "\n",
    "variable_name = {'fake' : 0 , 'real' : 1 }\n",
    "df2['type'] = df2['type'].map(variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[df2['text'].map(len) >= 1]\n",
    "#Reset index\n",
    "df2 = df2.reset_index().drop(\"index\", axis=1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting all to a new dataframe\n",
    "df3 = df2.loc[:,['title','text','type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the highest length of text and title:\n",
    "max_tokens_text = 13000\n",
    "max_tokens_title = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['text'] = list(pad_sequences((df2['text']), maxlen=max_tokens_text,padding='post'))\n",
    "df3['title'] = list(pad_sequences(df2['title'], maxlen=max_tokens_title,padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python object serialized\n",
    "df3.to_pickle('./model_data.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pickle = pd.read_pickle('./model_data.pickle')\n",
    "data_pickle = data_pickle.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train - test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_pickle[['text', 'title']], data_pickle['type'], test_size=0.2, random_state=42)\n",
    "\n",
    "#Train - valid\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42)\n",
    " \n",
    "\n",
    "train_fit = [np.asarray(X_train['title'].tolist()), \n",
    "             np.asarray(X_train['text'].tolist())\n",
    "             ]\n",
    "\n",
    "valid_fit = [np.asarray(X_valid['title'].tolist()), \n",
    "             np.asarray(X_valid['text'].tolist())\n",
    "             ]\n",
    "\n",
    "test_fit = [np.asarray(X_test['title'].tolist()), \n",
    "             np.asarray(X_test['text'].tolist())\n",
    "             ]    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "seed(50)\n",
    "tf.random.set_seed(50)\n",
    "k.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 48)]         0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 13000)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, 48, 300)      900000000   ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 13000, 300)   900000000   ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 48, 300)      0           ['embedding_6[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 13000, 300)   0           ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 23, 32)       38432       ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 6499, 64)     76864       ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 11, 32)      0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 1624, 64)    0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 352)          0           ['max_pooling1d_5[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 103936)       0           ['max_pooling1d_6[0][0]']        \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 50)           17650       ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 100)          10393700    ['flatten_6[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 150)          0           ['dense_9[0][0]',                \n",
      "                                                                  'dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 50)           7550        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 50)           0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1)            51          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,810,534,247\n",
      "Trainable params: 10,534,247\n",
      "Non-trainable params: 1,800,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_title\n",
    "title_input = k.layers.Input(shape=(max_tokens_title,))\n",
    "inp1 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(title_input)\n",
    "x1 = k.layers.Dropout(0.2)(inp1)\n",
    "\n",
    "#Added\n",
    "x1 = k.layers.Conv1D(filters = 32, kernel_size=4, strides=2, activation='relu')(x1)\n",
    "x1 = k.layers.MaxPool1D(pool_size = 2)(x1)\n",
    "x1 = k.layers.Flatten()(x1)\n",
    "x1 = k.layers.Dense(50, activation='relu', kernel_regularizer='l2')(x1)\n",
    "\n",
    "#input_Text\n",
    "text_input = k.layers.Input(shape=(max_tokens_text,))\n",
    "inp2 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(text_input)\n",
    "x2 = k.layers.Dropout(0.2)(inp2)\n",
    "\n",
    "x2 = k.layers.Conv1D(filters = 64, kernel_size = 4, strides = 2, activation='relu')(x2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4)(x2)\n",
    "\n",
    "#Added\n",
    "# x2 = k.layers.Conv1D(filters = 80, kernel_size = 16, strides = 4, activation='relu')(inp2)\n",
    "# x2 = k.layers.MaxPool1D(pool_size = 4)(x2)\n",
    "x2 = k.layers.Flatten()(x2)\n",
    "x2 = k.layers.Dense(100, activation='relu', kernel_regularizer='l2')(x2)\n",
    "\n",
    "\n",
    "#Merge\n",
    "x = k.layers.concatenate([x1, x2])\n",
    "\n",
    "#Common part\n",
    "# x = k.layers.Dense(50, activation='relu')(x)\n",
    "# x = k.layers.Dropout(0.2)(x)\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "out = k.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "#Build model\n",
    "model = k.models.Model(inputs=[title_input, text_input], outputs=[out])\n",
    "\n",
    "model.compile(k.optimizers.RMSprop(), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "99/99 [==============================] - 306s 3s/step - loss: 0.9944 - acc: 0.6905 - val_loss: 0.5832 - val_acc: 0.7697\n",
      "Epoch 2/20\n",
      "99/99 [==============================] - 309s 3s/step - loss: 0.5125 - acc: 0.8144 - val_loss: 0.4444 - val_acc: 0.8574\n",
      "Epoch 3/20\n",
      "99/99 [==============================] - 309s 3s/step - loss: 0.4329 - acc: 0.8575 - val_loss: 0.4179 - val_acc: 0.8605\n",
      "Epoch 4/20\n",
      "99/99 [==============================] - 308s 3s/step - loss: 0.3714 - acc: 0.8887 - val_loss: 0.3421 - val_acc: 0.8999\n",
      "Epoch 5/20\n",
      "99/99 [==============================] - 299s 3s/step - loss: 0.3187 - acc: 0.9142 - val_loss: 0.3424 - val_acc: 0.9003\n",
      "Epoch 6/20\n",
      "99/99 [==============================] - 289s 3s/step - loss: 0.2831 - acc: 0.9267 - val_loss: 0.3064 - val_acc: 0.9085\n",
      "Epoch 7/20\n",
      "99/99 [==============================] - 313s 3s/step - loss: 0.2595 - acc: 0.9364 - val_loss: 0.2881 - val_acc: 0.9196\n",
      "Epoch 8/20\n",
      "99/99 [==============================] - 298s 3s/step - loss: 0.2291 - acc: 0.9469 - val_loss: 0.2879 - val_acc: 0.9276\n",
      "Epoch 9/20\n",
      "99/99 [==============================] - 294s 3s/step - loss: 0.2137 - acc: 0.9550 - val_loss: 0.2544 - val_acc: 0.9314\n",
      "Epoch 10/20\n",
      "99/99 [==============================] - 288s 3s/step - loss: 0.1921 - acc: 0.9616 - val_loss: 0.2717 - val_acc: 0.9273\n",
      "Epoch 11/20\n",
      "99/99 [==============================] - 289s 3s/step - loss: 0.1830 - acc: 0.9648 - val_loss: 0.3687 - val_acc: 0.9003\n",
      "Epoch 12/20\n",
      "99/99 [==============================] - 290s 3s/step - loss: 0.1630 - acc: 0.9728 - val_loss: 0.2540 - val_acc: 0.9355\n",
      "Epoch 13/20\n",
      "99/99 [==============================] - 290s 3s/step - loss: 0.1510 - acc: 0.9769 - val_loss: 0.2964 - val_acc: 0.9133\n",
      "Epoch 14/20\n",
      "99/99 [==============================] - 290s 3s/step - loss: 0.1392 - acc: 0.9789 - val_loss: 0.2360 - val_acc: 0.9381\n",
      "Epoch 15/20\n",
      "99/99 [==============================] - 297s 3s/step - loss: 0.1367 - acc: 0.9800 - val_loss: 0.2438 - val_acc: 0.9406\n",
      "Epoch 16/20\n",
      "99/99 [==============================] - 306s 3s/step - loss: 0.1216 - acc: 0.9835 - val_loss: 0.2580 - val_acc: 0.9400\n",
      "Epoch 17/20\n",
      "99/99 [==============================] - 305s 3s/step - loss: 0.1113 - acc: 0.9883 - val_loss: 0.2342 - val_acc: 0.9409\n",
      "Epoch 18/20\n",
      "99/99 [==============================] - 300s 3s/step - loss: 0.1077 - acc: 0.9871 - val_loss: 0.2609 - val_acc: 0.9279\n",
      "Epoch 19/20\n",
      "99/99 [==============================] - 300s 3s/step - loss: 0.1082 - acc: 0.9867 - val_loss: 0.2247 - val_acc: 0.9444\n",
      "Epoch 20/20\n",
      "99/99 [==============================] - 301s 3s/step - loss: 0.1084 - acc: 0.9852 - val_loss: 0.2386 - val_acc: 0.9387\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=128, epochs=20,\n",
    "        callbacks = [k.callbacks.EarlyStopping(monitor='val_loss', patience=4)], \n",
    "        validation_data=(valid_fit, np.array(y_valid.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      2354\n",
      "           1       0.96      0.89      0.92      1581\n",
      "\n",
      "    accuracy                           0.94      3935\n",
      "   macro avg       0.94      0.93      0.94      3935\n",
      "weighted avg       0.94      0.94      0.94      3935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(np.array(y_test.tolist()), test_pred.round())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "seed(50)\n",
    "tf.random.set_seed(50)\n",
    "k.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_title\n",
    "title_input = k.layers.Input(shape=(max_tokens_title,))\n",
    "inp1 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(title_input)\n",
    "\n",
    "x = k.layers.Conv1D(filters = 5, kernel_size=4, strides=2, activation='relu')(inp1)\n",
    "x = k.layers.MaxPool1D(pool_size = 2)(x)\n",
    "x = k.layers.Flatten()(x)\n",
    "x = k.layers.Dense(50, activation='relu',  kernel_regularizer='l2')(x)\n",
    "\n",
    "#input_content\n",
    "text_input = k.layers.Input(shape=(max_tokens_text,))\n",
    "inp2 = k.layers.Embedding(input_dim=3000000,output_dim=300,trainable=False)(text_input)\n",
    "\n",
    "x2 = k.layers.Conv1D(filters = 40, kernel_size = 16, strides = 2, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4 )(x2)\n",
    "\n",
    "#Added\n",
    "x2 = k.layers.Conv1D(filters = 80, kernel_size = 16, strides = 4, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4, )(x2)\n",
    "x2 = k.layers.Flatten()(x2)\n",
    "x2 = k.layers.Dense(100, activation='relu', kernel_regularizer='l2')(x2)\n",
    "\n",
    "\n",
    "#Merge\n",
    "x = k.layers.concatenate([x, x2])\n",
    "\n",
    "#Common part\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "out = k.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "\n",
    "#Build model\n",
    "model = k.models.Model(inputs=[title_input, text_input], outputs=[out])\n",
    "\n",
    "model.compile(k.optimizers.RMSprop(), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=128, epochs=30,\n",
    "        callbacks = [k.callbacks.EarlyStopping(monitor='val_loss', patience=4)], \n",
    "        validation_data=(valid_fit, np.array(y_valid.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted indices (ValueError: Classification metrics can't handle a mix of binary and continuous-multioutput targets)\n",
    "test_pred = np.argmax(test_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(np.array(y_test.tolist()), test_pred.round())\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c79634922abe25a0b9d55bc2eb6abac259bd6c8bbb4022582053c5e5bdfcda26"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
