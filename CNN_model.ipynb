{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comet_ml at the top of your file\n",
    "# from comet_ml import Experiment\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# from matplotlib import style\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import swifter\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras as k\n",
    "import tensorflow as tf\n",
    "# import torch\n",
    "from scipy import stats\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Create an experiment with your api key\n",
    "# experiment = Experiment(\n",
    "#     api_key=\"BKac2uRt0FMAlheXf6HClaZhD\",\n",
    "#     project_name=\"general\",\n",
    "#     workspace=\"sarthak7034\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('gpu')))\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/s/sarthak_7034/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the confusion matrix (code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "data = pd.read_csv(\"./all_data.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    #Tokenize\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # remove stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]     # Experiment with removing it!!!\n",
    "    #Remove non alphanumerica characters\n",
    "    words = [word for word in tokens if word.isalpha()]    \n",
    "    return words\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49249ecceb65416592e47e1252146671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/20015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea16f5c933c4c9a8ea24673582526e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/20015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying lamdba function to clean text\n",
    "data['text'] = data.swifter.apply(lambda row: clean_text(row['text']), axis=1)\n",
    "#Clean title\n",
    "data['title'] = data.swifter.apply(lambda row: clean_text(row['title']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing row with only text\n",
    "data = data[data['title'].str.len() >= 1]\n",
    "data = data[data['text'].str.len() >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = data.loc[:,['anger','anticipation','disgust','fear','joy','sadness','surprise','trust','negative','positive']]\n",
    "\n",
    "# # Zero Score Dataframe Normalization\n",
    "# MaxMin_df = stats.zscore(df1)\n",
    "# MaxMin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the new dataset which we will be working with\n",
    "df2 = data.loc[:,['title','text','type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether any values are missing\n",
    "# df2.isnull().sum()\n",
    "# df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging columns\n",
    "# df2['content'] = df2['title'] + df2['text']\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[At, Donald, Trump, Properties, Showcase, Bran...</td>\n",
       "      <td>[They, stood, line, Trump, Tower, sometimes, h...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Trump, Foundation, Tells, New, York, It, Has,...</td>\n",
       "      <td>[Donald, J, Trump, foundation, informed, Attor...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Donald, Trump, Prepares, White, House, Move, ...</td>\n",
       "      <td>[President, Donald, J, Trump, White, House, ou...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Luring, Chinese, Investors, With, Trump, Name...</td>\n",
       "      <td>[An, investment, pitch, new, Texas, hotel, try...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Melania, Barron, Trump, Won, Immediately, Mov...</td>\n",
       "      <td>[President, Donald, J, Trump, wife, Melania, s...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19800</th>\n",
       "      <td>[Alabama, Lawmaker, Same, Couples, Don, Deserv...</td>\n",
       "      <td>[Most, conservatives, oppose, marriage, equali...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19801</th>\n",
       "      <td>[GOP, Senator, David, Perdue, Jokes, About, Pr...</td>\n",
       "      <td>[The, freshman, senator, Georgia, quoted, scri...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19802</th>\n",
       "      <td>[State, Department, says, find, emails, Clinto...</td>\n",
       "      <td>[The, State, Department, told, Republican, Nat...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19803</th>\n",
       "      <td>[In, Ethiopia, Obama, seeks, progress, peace, ...</td>\n",
       "      <td>[ADDIS, ABABA, Ethiopia, Obama, convened, meet...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19804</th>\n",
       "      <td>[Jeb, Bush, Is, Suddenly, Attacking, Trump, He...</td>\n",
       "      <td>[Jeb, Bush, Is, Suddenly, Attacking, Trump, He...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19805 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      [At, Donald, Trump, Properties, Showcase, Bran...   \n",
       "1      [Trump, Foundation, Tells, New, York, It, Has,...   \n",
       "2      [Donald, Trump, Prepares, White, House, Move, ...   \n",
       "3      [Luring, Chinese, Investors, With, Trump, Name...   \n",
       "4      [Melania, Barron, Trump, Won, Immediately, Mov...   \n",
       "...                                                  ...   \n",
       "19800  [Alabama, Lawmaker, Same, Couples, Don, Deserv...   \n",
       "19801  [GOP, Senator, David, Perdue, Jokes, About, Pr...   \n",
       "19802  [State, Department, says, find, emails, Clinto...   \n",
       "19803  [In, Ethiopia, Obama, seeks, progress, peace, ...   \n",
       "19804  [Jeb, Bush, Is, Suddenly, Attacking, Trump, He...   \n",
       "\n",
       "                                                    text  type  \n",
       "0      [They, stood, line, Trump, Tower, sometimes, h...  real  \n",
       "1      [Donald, J, Trump, foundation, informed, Attor...  real  \n",
       "2      [President, Donald, J, Trump, White, House, ou...  real  \n",
       "3      [An, investment, pitch, new, Texas, hotel, try...  real  \n",
       "4      [President, Donald, J, Trump, wife, Melania, s...  real  \n",
       "...                                                  ...   ...  \n",
       "19800  [Most, conservatives, oppose, marriage, equali...  real  \n",
       "19801  [The, freshman, senator, Georgia, quoted, scri...  real  \n",
       "19802  [The, State, Department, told, Republican, Nat...  real  \n",
       "19803  [ADDIS, ABABA, Ethiopia, Obama, convened, meet...  real  \n",
       "19804  [Jeb, Bush, Is, Suddenly, Attacking, Trump, He...  real  \n",
       "\n",
       "[19805 rows x 3 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2[df2['text'].map(len) >= 1]\n",
    "#Reset index\n",
    "df2 = df2.reset_index().drop(\"index\", axis=1)\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "fake    11731\n",
      "real     8074\n",
      "Name: title, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How many fake and real articles?\n",
    "print(df2.groupby(['type'])['title'].count())\n",
    "df2.groupby(['type'])['title'].count().plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed = KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(model_embed.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Matrix\n",
    "embed_mat = np.zeros((50000,300))\n",
    "for i, e in model_embed.key_to_index.items():\n",
    "    # embed_mat[model_embed.key_to_index['e']] = model_embed[i]\n",
    "    embed_mat[model_embed.get_index('e')] = model_embed[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840f1134229241429d4c36ab1e5d41eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e204c2f7637942ef9c4f275b82008065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying it to the columns \n",
    "\n",
    "df2['title'] = df2.swifter.apply(lambda r: [model_embed.key_to_index[x] for x in r['title'] if x in model_embed.key_to_index], axis=1)\n",
    "df2['text'] = df2.swifter.apply(lambda r: [model_embed.key_to_index[x] for x in r['text'] if x in model_embed.key_to_index], axis=1)\n",
    "# df2['content'] = df2.apply(lambda r: [model_embed.key_to_index[x] for x in r['content'] if x in model_embed.key_to_index], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6f5b347e7c442e97bb250007e8b27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "encoder = LabelBinarizer().fit(list(df2['type']))\n",
    "df2['type'] = df2.swifter.apply(lambda r: encoder.transform([r['type']])[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[332, 6117, 13034, 11062, 20257, 9193, 446]</td>\n",
       "      <td>[128, 2379, 318, 13034, 9413, 1631, 242, 957, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[13034, 2124, 44625, 442, 5353, 51, 7754, 8808...</td>\n",
       "      <td>[6117, 4883, 13034, 3113, 2826, 4605, 1927, 33...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6117, 13034, 49031, 1790, 659, 11977, 553, 94...</td>\n",
       "      <td>[446, 6117, 4883, 13034, 1790, 659, 16780, 177...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[176192, 1035, 3789, 316, 13034, 12441, 3337, ...</td>\n",
       "      <td>[741, 615, 1924, 65, 675, 1639, 469, 8793, 112...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[282726, 14896, 13034, 26994, 17785, 11977, 17...</td>\n",
       "      <td>[446, 6117, 4883, 13034, 783, 282726, 831, 148...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19800</th>\n",
       "      <td>[2625, 47190, 14968, 22328, 4119, 115896, 1496...</td>\n",
       "      <td>[1214, 6555, 5472, 2724, 10729, 12687, 6484, 3...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19801</th>\n",
       "      <td>[2751, 4301, 689, 19370, 82238, 1523, 94624, 4...</td>\n",
       "      <td>[7, 2779, 4264, 1742, 2717, 38413, 168, 1956, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19802</th>\n",
       "      <td>[268, 423, 115, 359, 9451, 1820, 1735, 3803]</td>\n",
       "      <td>[7, 268, 423, 162, 917, 430, 1468, 75, 359, 94...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19803</th>\n",
       "      <td>[70, 9711, 494, 3786, 1463, 1356, 371, 1063, 1...</td>\n",
       "      <td>[1538939, 1151610, 9711, 494, 13595, 349, 618,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19804</th>\n",
       "      <td>[69891, 513, 1732, 13741, 59535, 13034, 2165, ...</td>\n",
       "      <td>[69891, 513, 1732, 13741, 59535, 13034, 2165, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19805 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0            [332, 6117, 13034, 11062, 20257, 9193, 446]   \n",
       "1      [13034, 2124, 44625, 442, 5353, 51, 7754, 8808...   \n",
       "2      [6117, 13034, 49031, 1790, 659, 11977, 553, 94...   \n",
       "3      [176192, 1035, 3789, 316, 13034, 12441, 3337, ...   \n",
       "4      [282726, 14896, 13034, 26994, 17785, 11977, 17...   \n",
       "...                                                  ...   \n",
       "19800  [2625, 47190, 14968, 22328, 4119, 115896, 1496...   \n",
       "19801  [2751, 4301, 689, 19370, 82238, 1523, 94624, 4...   \n",
       "19802       [268, 423, 115, 359, 9451, 1820, 1735, 3803]   \n",
       "19803  [70, 9711, 494, 3786, 1463, 1356, 371, 1063, 1...   \n",
       "19804  [69891, 513, 1732, 13741, 59535, 13034, 2165, ...   \n",
       "\n",
       "                                                    text type  \n",
       "0      [128, 2379, 318, 13034, 9413, 1631, 242, 957, ...  [1]  \n",
       "1      [6117, 4883, 13034, 3113, 2826, 4605, 1927, 33...  [1]  \n",
       "2      [446, 6117, 4883, 13034, 1790, 659, 16780, 177...  [1]  \n",
       "3      [741, 615, 1924, 65, 675, 1639, 469, 8793, 112...  [1]  \n",
       "4      [446, 6117, 4883, 13034, 783, 282726, 831, 148...  [1]  \n",
       "...                                                  ...  ...  \n",
       "19800  [1214, 6555, 5472, 2724, 10729, 12687, 6484, 3...  [1]  \n",
       "19801  [7, 2779, 4264, 1742, 2717, 38413, 168, 1956, ...  [1]  \n",
       "19802  [7, 268, 423, 162, 917, 430, 1468, 75, 359, 94...  [1]  \n",
       "19803  [1538939, 1151610, 9711, 494, 13595, 349, 618,...  [1]  \n",
       "19804  [69891, 513, 1732, 13741, 59535, 13034, 2165, ...  [1]  \n",
       "\n",
       "[19805 rows x 3 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd72e99ff7845c68e7c64038adcec5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca21d02c596432ea1061728932b447d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365 15\n"
     ]
    }
   ],
   "source": [
    "# Padding:----\n",
    "def pad_array(array, token_len):\n",
    "    diff_token = token_len - len(array)\n",
    "    if(diff_token < 0):\n",
    "        array = array[:token_len] \n",
    "    else:\n",
    "        array += [0]*diff_token \n",
    "        \n",
    "    return array \n",
    "\n",
    "tokens_numbers_text = df2.swifter.apply(lambda row: len(row['text']), axis = 1)\n",
    "max_tokens_text = int(np.mean(tokens_numbers_text) + 2 * np.std(tokens_numbers_text))\n",
    "\n",
    "tokens_numbers_title = df2.swifter.apply(lambda row: len(row['title']), axis = 1)\n",
    "max_tokens_title = int(np.mean(tokens_numbers_title) + 2 * np.std(tokens_numbers_title))\n",
    "print(max_tokens_text,max_tokens_title)    \n",
    "\n",
    "# # Check the highest length of text and title:\n",
    "# max_tokens_text = 13000\n",
    "# max_tokens_title = 48\n",
    "\n",
    "# Shifting all to a new dataframe\n",
    "df3 = df2.loc[:,['title','text','type']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80470e6c08a3453a8951b769eb88b8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15dcbef7d9384cebb40ff69d9bdd5727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df3['text'] = df2.swifter.apply(lambda r: pad_array(r['text'], max_tokens_text) , axis=1)\n",
    "df3['title'] = df2.swifter.apply(lambda r: pad_array(r['title'], max_tokens_title) , axis=1)\n",
    "\n",
    "\n",
    "# df3['text'] = list(pad_sequences((df2['text']), maxlen=max_tokens_text,padding='post'))\n",
    "# df3['title'] = list(pad_sequences(df2['title'], maxlen=max_tokens_title,padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python object serialized\n",
    "df3.to_pickle('./model_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pickle = pd.read_pickle('./model_data.pickle')\n",
    "data_pickle = data_pickle.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train - test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_pickle[['text', 'title']], data_pickle['type'], test_size=0.2, random_state=42)\n",
    "\n",
    "#Train - valid\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42)\n",
    " \n",
    "\n",
    "train_fit = [np.asarray(X_train['title'].tolist()), \n",
    "             np.asarray(X_train['text'].tolist())\n",
    "             ]\n",
    "\n",
    "valid_fit = [np.asarray(X_valid['title'].tolist()), \n",
    "             np.asarray(X_valid['text'].tolist())\n",
    "             ]\n",
    "\n",
    "test_fit = [np.asarray(X_test['title'].tolist()), \n",
    "             np.asarray(X_test['text'].tolist())\n",
    "             ]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12675, 15)\n",
      "(12675, 1365)\n"
     ]
    }
   ],
   "source": [
    "print(train_fit[0].shape)\n",
    "print(train_fit[1].shape)\n",
    "# print(valid_fit.shape)\n",
    "# print(test_fit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12675, 1)\n"
     ]
    }
   ],
   "source": [
    "gg=np.asarray(y_train.tolist())\n",
    "print(gg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "seed(50)\n",
    "tf.random.set_seed(50)\n",
    "k.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims=300\n",
    "num_steps = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 15)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1365)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 15, 300)      900000000   ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1365, 300)    900000000   ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 6, 5)         6005        ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 338, 80)      384080      ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " Pool1Title (MaxPooling1D)      (None, 3, 5)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 84, 80)      0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 15)           0           ['Pool1Title[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 6720)         0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " Dense1Title (Dense)            (None, 50)           800         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " Dense1Text (Dense)             (None, 100)          672100      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 150)          0           ['Dense1Title[0][0]',            \n",
      "                                                                  'Dense1Text[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           7550        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 50)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 50)           2550        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            51          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,801,073,136\n",
      "Trainable params: 1,073,136\n",
      "Non-trainable params: 1,800,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_title\n",
    "title_input = k.layers.Input(shape=(max_tokens_title,))\n",
    "# inp1 = k.layers.Embedding(input_dim=3000000,output_dim=300)(title_input)\n",
    "inp1 = k.layers.Embedding(input_dim=3000000,output_dim=300,input_length=num_steps,trainable=False)(title_input)\n",
    "\n",
    "#Added\n",
    "x = k.layers.Conv1D(filters = 5, kernel_size=4, strides=2, activation='relu')(inp1)\n",
    "x = k.layers.MaxPool1D(pool_size = 2, name='Pool1Title')(x)\n",
    "x = k.layers.Flatten()(x)\n",
    "x = k.layers.Dense(50, activation='relu', name='Dense1Title', kernel_regularizer='l2')(x)\n",
    "\n",
    "#input_Text\n",
    "text_input = k.layers.Input(shape=(max_tokens_text,))\n",
    "# inp2 = k.layers.Embedding(input_dim=3000000,output_dim=300)(text_input)\n",
    "inp2 = k.layers.Embedding(input_dim=3000000,output_dim=300,input_length=num_steps,trainable=False)(text_input)\n",
    "\n",
    "x2 = k.layers.Conv1D(filters = 40, kernel_size = 16, strides = 2, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4)(x2)\n",
    "\n",
    "#Added\n",
    "x2 = k.layers.Conv1D(filters = 80, kernel_size = 16, strides = 4, activation='relu')(inp2)\n",
    "x2 = k.layers.MaxPool1D(pool_size = 4)(x2)\n",
    "x2 = k.layers.Flatten()(x2)\n",
    "x2 = k.layers.Dense(100, activation='relu', kernel_regularizer='l2', name='Dense1Text')(x2)\n",
    "\n",
    "\n",
    "#Merge\n",
    "x = k.layers.concatenate([x, x2])\n",
    "\n",
    "#Common part\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "x = k.layers.Dense(50, activation='relu')(x)\n",
    "x = k.layers.Dropout(0.2)(x)\n",
    "out = k.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "#Build model\n",
    "model = k.models.Model(inputs=[title_input, text_input], outputs=[out])\n",
    "\n",
    "model.compile(k.optimizers.RMSprop(), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #input_title\n",
    "# title_input = k.layers.Input(shape=(max_tokens_title,), name='title_input')\n",
    "# inp = k.layers.Embedding(output_dim=300, input_dim=50000,)(title_input)\n",
    "\n",
    "# x = k.layers.Conv1D(filters = 5, kernel_size=4, strides=2, activation='relu', name='Conv1Title')(inp)\n",
    "# x = k.layers.MaxPool1D(pool_size = 2, name='Pool1Title')(x)\n",
    "# x = k.layers.Flatten()(x)\n",
    "# x = k.layers.Dense(50, activation='relu', name='Dense1Title', kernel_regularizer='l2')(x)\n",
    "\n",
    "# #input_content\n",
    "# content_input = k.layers.Input(shape=(max_tokens_text,), name='content_input')\n",
    "# inp2 = k.layers.Embedding(output_dim=300, input_dim=50000)(content_input)\n",
    "# x2 = k.layers.Conv1D(filters = 40, kernel_size = 16, strides = 2, activation='relu', name='Conv1Content')(inp2)\n",
    "# x2 = k.layers.MaxPool1D(pool_size = 4, name='Pool1Content')(x2)\n",
    "\n",
    "# #Added\n",
    "# x2 = k.layers.Conv1D(filters = 80, kernel_size = 16, strides = 4, activation='relu', name='Conv2Content')(inp2)\n",
    "# x2 = k.layers.MaxPool1D(pool_size = 4, name='Pool2Content')(x2)\n",
    "# x2 = k.layers.Flatten()(x2)\n",
    "# x2 = k.layers.Dense(100, activation='relu', kernel_regularizer='l2', name='Dense1Content')(x2)\n",
    "\n",
    "\n",
    "# #Merge\n",
    "# x = k.layers.concatenate([x, x2])\n",
    "\n",
    "# #Common part\n",
    "# x = k.layers.Dense(50, activation='relu')(x)\n",
    "# x = k.layers.Dropout(0.2)(x)\n",
    "# x = k.layers.Dense(50, activation='relu')(x)\n",
    "# x = k.layers.Dropout(0.2)(x)\n",
    "# out = k.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "\n",
    "# #Build model\n",
    "# model = k.models.Model(inputs=[title_input, content_input], outputs=[out])\n",
    "\n",
    "# model.compile(k.optimizers.RMSprop(), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 21s 186ms/step - loss: 0.8356 - acc: 0.7118 - val_loss: 0.5592 - val_acc: 0.8220\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 18s 185ms/step - loss: 0.4504 - acc: 0.8539 - val_loss: 0.8031 - val_acc: 0.6967\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 18s 185ms/step - loss: 0.3277 - acc: 0.8970 - val_loss: 0.3910 - val_acc: 0.8653\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 19s 186ms/step - loss: 0.2473 - acc: 0.9294 - val_loss: 0.3583 - val_acc: 0.8896\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 18s 185ms/step - loss: 0.1672 - acc: 0.9644 - val_loss: 0.3657 - val_acc: 0.8940\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 0.1284 - acc: 0.9761 - val_loss: 0.3770 - val_acc: 0.8959\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=128, epochs=20,\n",
    "        callbacks = [k.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta = 0.1)], \n",
    "        validation_data=(valid_fit, np.array(y_valid.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion Matrix of SVM\n",
    "# cm = metrics.confusion_matrix(y_test, prediction)\n",
    "# plot_confusion_matrix(cm, classes=['Fake', 'Real'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c79634922abe25a0b9d55bc2eb6abac259bd6c8bbb4022582053c5e5bdfcda26"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
